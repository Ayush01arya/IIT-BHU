{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d39d38c-1dc3-4a91-8dff-226a300117ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to 'word_data.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Input data\n",
    "data =  \n",
    "    \"ladZikA\", \"ladZikA\", \"ladZikana\", \"ladZikA\", \"ladZikana\",\n",
    "    \"rAjA\", \"rAjA\", \"rAjA\", \"rAjA\", \"rAjana\",\n",
    "    \"Gara\", \"Gara\", \"Gara\", \"Gara\", \"Garana\",\n",
    "    \"Karca\", \"Karca\", \"Karca\", \"Karca\", \"Karcana\",\n",
    "    \"kavi\", \"kavi\", \"kavi\", \"kavi\", \"kavina\",\n",
    "    \"AxamI\", \"AxamI\", \"AxamI\", \"AxamI\", \"Axamina\",\n",
    "    \"sawru\", \"sawru\", \"sawru\", \"sawru\", \"sawruna\",\n",
    "    \"AlU\", \"AlU\", \"AlU\", \"AlU\", \"Aluna\",\n",
    "    \"kuvAz\", \"kuvAz/kuAz/kuvAM/kuAM\", \"kuvAz/kuAz/kuvAM/kuAM\", \"kuvAz/kuAz/kuvAM/kuAM\", \"kuvazna/kuvaMna/kuazna/kuaMna\",\n",
    "    \"gehUz\", \"gehUz/gehUM\", \"gehUz/gehUM\", \"gehUz/gehUM\", \"gehuzna/gehuMna\",\n",
    "    \"kroXa\", \"kroXa\", \"kroXa\", \"kroXa\", \"kroXa\",\n",
    "    \"lakaTo\", \"lakaTo\", \"lakaTana/lakaTona\", \"lakaTo\", \"lakaTana/lakaTona\",\n",
    "    \"sarasoM\", \"sarasoM/sarasoz\", \"sarasoMna/sarasozna/sarasaMna/sarasazna\", \"sarasoM/sarasoz\", \"sarasoMna/sarasozna/sarasaMna/sarasazna\",\n",
    "    \"gosAIM\", \"gosAIM/gosAIz\", \"gosAIM/gosAIz\", \"gosAIM/gosAIz\", \"gosAIMana/gosAIzana\",\n",
    "    \"sonarA\", \"sonarA\", \"sonarA\", \"sonarA\", \"sonarana\"\n",
    " \n",
    "\n",
    "# Prepare the output structure\n",
    "categories =   \n",
    "word_roots =   \n",
    "word_forms =   \n",
    "\n",
    "# Process the input data\n",
    "for i in range(0, len(data), 5):\n",
    "    word_root = data i \n",
    "    word_form1 = data i + 1 \n",
    "    word_form2 = data i + 2 \n",
    "    word_form3 = data i + 3 \n",
    "    word_form4 = data i + 4 \n",
    "    \n",
    "    categories.append(\"Noun_m\")  # Assuming all are masculine nouns\n",
    "    word_roots.append(word_root)\n",
    "    word_forms.append( word_form1, word_form2, word_form3, word_form4 )\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Category': categories,\n",
    "    'Word Root': word_roots,\n",
    "    'Word Form 1':  forms 0  for forms in word_forms ,\n",
    "    'Word Form 2':  forms 1  for forms in word_forms ,\n",
    "    'Word Form 3':  forms 2  for forms in word_forms ,\n",
    "    'Word Form 4':  forms 3  for forms in word_forms ,\n",
    "})\n",
    "\n",
    "# Save to Excel\n",
    "df.to_excel('word_data1.xlsx', index=False)\n",
    "\n",
    "print(\"Data has been saved to 'word_data.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18f9d05c-7e6f-4cf6-959e-051f7ab251c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to 'word_data_new.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Input data (from noun_me_e.txt)\n",
    "data =  \n",
    "    \"ladZikA\", \"ladZike/ladZikai/ladZikE\", \"ladZikane/ladZikanai/ladZikanE\", \"ladZike/ladZikai/ladZikE\", \"ladZikane/ladZikanai/ladZikanE\",\n",
    "    \"rAjA\", \"rAje/rAjai/rAjE\", \"rAje/rAjai/rAjE\", \"rAje/rAjai/rAjE\", \"rAjanai/rAjane/rAjanE\",\n",
    "    \"Gara\", \"Gare/Garai/GarE\", \"Gare/Garai/GarE\", \"Gare/Garai/GarE\", \"Garanai/Garane/GaranE\",\n",
    "    \"Karca\", \"Karce/Karcai/KarcE\", \"Karce/Karcai/KarcE\", \"Karce/Karcai/KarcE\", \"Karcanai/Karcane/KarcanE\",\n",
    "    \"kavi\", \"kavie/kaviai/kaviE\", \"kavie/kaviai/kaviE\", \"kavie/kaviai/kaviE\", \"kavine/kavinai/kavinE\",\n",
    "    \"AxamI\", \"Axamie/Axamiai/AxamiE\", \"Axamie/Axamiai/AxamiE\", \"Axamie/Axamiai/AxamiE\", \"Axaminai/Axamine/AxaminE\",\n",
    "    \"sawru\", \"sawrue/sawruai/sawruE\", \"sawrue/sawruai/sawruE\", \"sawrue/sawruai/sawruE\", \"sawrune/sawrunai/sawrunE\",\n",
    "    \"AlU\", \"Alue/Aluai/AluE\", \"Alue/Aluai/AluE\", \"Alue/Aluai/AluE\", \"Alunai/Alune/AlunE\",\n",
    "    \"kuvAz\", \"kuveM/kuvEM/kuvaiM/kueM/kuvEM/kuvaiM/kuvez/kuvEz/kuvazi/kuez/kuEZ/kuaiz\", \n",
    "    \"kuveM/kuvEM/kuvaiM/kueM/kuvEM/kuvaiM/kuvez/kuvEz/kuvazi/kuez/kuEZ/kuaiz\", \n",
    "    \"kuveM/kuvEM/kuvaiM/kueM/kuvEM/kuvaiM/kuvez/kuvEz/kuvazi/kuez/kuEZ/kuaiz\", \n",
    "    \"kuvaznai/kuvaMnai/kuMvane/kuMvanE/kuzvane/kuzvanE/kuazne/kuaMne/kuaznai/kuaMnai/kuaznE/kuaMnE\",\n",
    "    \"gehUz\", \"gehUze/gehUMe/gehUzE/gehUME/gehUzai/gehUMai\", \"gehUze/gehUMe/gehUzE/gehUME/gehUzai/gehUMai\", \n",
    "    \"gehUze/gehUMe/gehUzE/gehUME/gehUzai/gehUMai\", \"gehuzne/gehuMne/gehuznai/gehuMnai/gehuznE/gehuMnE\",\n",
    "    \"kroXa\", \"kroXe/kroXE/kroXai\", \"kroXe/kroXE/kroXai\", \"kroXe/kroXE/kroXai\", \"kroXe/kroXE/kroXai\",\n",
    "    \"lakaTo\", \"lakaTe/lakaTai/lakaTE\", \"lakaTane/lakaTone/lakaTanai/lakaTonai/lakaTanE/lakaTonE\", \n",
    "    \"lakaTe/lakaTai/lakaTE\", \"lakaTane/lakaTone/lakaTanai/lakaTonai/lakaTanE/lakaTonE\",\n",
    "    \"sarasoM\", \"sarasoMeM/sarasozez/sarasoMEM/sarasozEz/sarasoMaiM/sarasozaiz\", \n",
    "    \"sarasoMne/sarasoznai/sarasaMnai/sarasaznai/sarasaMnE/sarasaznE\", \n",
    "    \"sarasoMeM/sarasozez/sarasoMEM/sarasozEz/sarasoMaiM/sarasozaiz\", \n",
    "    \"sarasoMne/sarasozne/sarasaMnai/sarasaznai/sarasaMnE/sarasaznE\",\n",
    "    \"gosAIM\", \"gosAIMe/gosAIze/gosAIMaiM/gosAIzaiz/gosAIMEM/gosAIzEz\", \n",
    "    \"gosAIMe/gosAIze/gosAIMaiM/gosAIzaiz/gosAIMEM/gosAIzEz\", \n",
    "    \"gosAIMe/gosAIze/gosAIMaiM/gosAIzaiz/gosAIMEM/gosAIzEz\", \n",
    "    \"gosAIManai/gosAIzanai/gosAIMane/gosAIzane/gosAIManE/gosAIzanE\",\n",
    "    \"sonarA\", \"sonare/sonarai/sonarE\", \"sonare/sonarai/sonarE\", \"sonare/sonarai/sonarE\", \"sonaranai/sonarane/sonaranE\"\n",
    " \n",
    "\n",
    "# Prepare the output structure\n",
    "categories =   \n",
    "word_roots =   \n",
    "word_forms =   \n",
    "\n",
    "# Process the input data\n",
    "for i in range(0, len(data), 5):\n",
    "    word_root = data i \n",
    "    word_form1 = data i + 1 \n",
    "    word_form2 = data i + 2 \n",
    "    word_form3 = data i + 3 \n",
    "    word_form4 = data i + 4 \n",
    "    \n",
    "    categories.append(\"Noun_m_e\")  # Based on the new file\n",
    "    word_roots.append(word_root)\n",
    "    word_forms.append( word_form1, word_form2, word_form3, word_form4 )\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Category': categories,\n",
    "    'Word Root': word_roots,\n",
    "    'Word Form 1':  forms 0  for forms in word_forms ,\n",
    "    'Word Form 2':  forms 1  for forms in word_forms ,\n",
    "    'Word Form 3':  forms 2  for forms in word_forms ,\n",
    "    'Word Form 4':  forms 3  for forms in word_forms ,\n",
    "})\n",
    "\n",
    "# Save to Excel\n",
    "df.to_excel('word_data_new.xlsx', index=False)\n",
    "\n",
    "print(\"Data has been saved to 'word_data_new.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1de245-f882-494d-964d-d66b67a80f99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bea91fa9-4e0a-4661-9937-9638363cd6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to 'word_data_noun_m_e1.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Input data\n",
    "data =  \n",
    "    \"ladZikA\", \"ladZiko/ladZikO/ladZikau\", \"ladZikano/ladZikanO/ladZikanau\", \"ladZiko/ladZikO/ladZikau\", \"ladZikanO/ladZikano/ladZikanau\",\n",
    "    \"rAjA\", \"rAjo/rAjO/rAjau\", \"rAjo/rAjO/rAjau\", \"rAjo/rAjO/rAjau\", \"rAjano/rAjanO/rAjanau\",\n",
    "    \"Gara\", \"Garo/GarO/Garau\", \"Garo/GarO/Garau\", \"Garo/GarO/Garau\", \"Garano/GaranO/Garanau\",\n",
    "    \"Karca\", \"Karco/KarcO/Karcau\", \"Karco/KarcO/Karcau\", \"Karco/KarcO/Karcau\", \"Karcano/KarcanO/Karcanau\",\n",
    "    \"kavi\", \"kavio/kaviau/kaviO\", \"kavio/kaviau/kaviO\", \"kavio/kaviau/kaviO\", \"kavino/kavinau/kavinO\",\n",
    "    \"AxamI\", \"Axamio/Axamiau/AxamiO\", \"Axamio/Axamiau/AxamiO\", \"Axamio/Axamiau/AxamiO\", \"Axaminau/Axamino/AxaminO\",\n",
    "    \"sawru\", \"sawruo/sawruau/sawruO\", \"sawruo/sawruau/sawruO\", \"sawruo/sawruau/sawruO\", \"sawruno/sawrunau/sawrunO\",\n",
    "    \"AlU\", \"Aluo/Aluau/AluO\", \"Aluo/Aluau/AluO\", \"Aluo/Aluau/AluO\", \"Alunau/Aluno/AlunO\",\n",
    "    \"kuvAz\", \"kuvoM/kuvOM/kuvauM/kuvoz/kuvOz/kuvauz/kuoM/kuOM/kuaMu/kuoz/kuOz/kuazu\", \"kuvoM/kuvOM/kuvauM/kuvoz/kuvOz/kuvauz/kuoM/kuOM/kuaMu/kuoz/kuOz/kuazu\", \n",
    "    \"kuvoM/kuvOM/kuvauM/kuvoz/kuvOz/kuvauz/kuoM/kuOM/kuaMu/kuoz/kuOz/kuazu\", \"kuvaznau/kuvaMnau/kuMvano/kuMvanO/kuzvano/kuzvanO/kuazno/kuaMno/kuaznau/kuaMnau/kuaznO/kuaMnO\",\n",
    "    \"gehUz\", \"gehUzo/gehUMo/gehUzO/gehUMO/gehUzau/gehUMau\", \"gehUzo/gehUMo/gehUzO/gehUMO/gehUzau/gehUMau\", \"gehUzo/gehUMo/gehUzO/gehUMO/gehUzau/gehUMau\", \n",
    "    \"gehuzno/gehuMno/gehuznau/gehuMnau/gehuznO/gehuMnO\", \n",
    "    \"kroXa\", \"kroXo/kroXO/kroXau\", \"kroXo/kroXO/kroXau\", \"kroXo/kroXO/kroXau\", \"kroXo/kroXO/kroXau\",\n",
    "    \"lakaTo\", \"lakaTo/lakaTau/lakaTO\", \"lakaTano/lakaTono/lakaTanau/lakaTonau/lakaTanO/lakaTonO\", \"lakaTo/lakaTau/lakaTO\", \n",
    "    \"lakaTano/lakaTono/lakaTanau/lakaTonau/lakaTanO/lakaTonO\",\n",
    "    \"sarasoM\", \"sarasoMoM/sarasozoz/sarasoMOM/sarasozOz/sarasoMauM/sarasozauz\", \"sarasoMno/sarasoznau/sarasaMnau/sarasaznau/sarasaMnO/sarasaznO\", \n",
    "    \"sarasoMoM/sarasozoz/sarasoMOM/sarasozOz/sarasoMauM/sarasozauz\", \"sarasoMno/sarasozno/sarasaMnau/sarasaznau/sarasaMnO/sarasaznO\",\n",
    "    \"gosAIM\", \"gosAIMo/gosAIzo/gosAIMauM/gosAIzauz/gosAIMOM/gosAIzOz\", \"gosAIMo/gosAIzo/gosAIMauM/gosAIzauz/gosAIMOM/gosAIzOz\", \n",
    "    \"gosAIMo/gosAIzo/gosAIMauM/gosAIzauz/gosAIMOM/gosAIzOz\", \"gosAIManau/gosAIzanau/gosAIMano/gosAIzano/gosAIManO/gosAIzanO\",\n",
    "    \"sonarA\", \"sonaro/sonarau/sonarO\", \"sonaro/sonarau/sonarO\", \"sonaro/sonarau/sonarO\", \"sonaranau/sonarano/sonaranO\"\n",
    " \n",
    "\n",
    "# Prepare the output structure\n",
    "categories =   \n",
    "word_roots =   \n",
    "word_forms =   \n",
    "\n",
    "# Process the input data\n",
    "for i in range(0, len(data), 5):\n",
    "    word_root = data i \n",
    "    word_form1 = data i + 1 \n",
    "    word_form2 = data i + 2 \n",
    "    word_form3 = data i + 3 \n",
    "    word_form4 = data i + 4 \n",
    "    \n",
    "    categories.append(\"Noun_m_e1\")  # New category for this dataset\n",
    "    word_roots.append(word_root)\n",
    "    word_forms.append( word_form1, word_form2, word_form3, word_form4 )\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Category': categories,\n",
    "    'Word Root': word_roots,\n",
    "    'Word Form 1':  forms 0  for forms in word_forms ,\n",
    "    'Word Form 2':  forms 1  for forms in word_forms ,\n",
    "    'Word Form 3':  forms 2  for forms in word_forms ,\n",
    "    'Word Form 4':  forms 3  for forms in word_forms ,\n",
    "})\n",
    "\n",
    "# Save to Excel\n",
    "df.to_excel('word_data_noun_m_e1.xlsx', index=False)\n",
    "\n",
    "print(\"Data has been saved to 'word_data_noun_m_e1.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7caf8507-396f-4820-a594-654d88172072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to 'word_data_noun_m_e1.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Input data\n",
    "data =  \n",
    "    \"ladZakI\", \"ladZakI\", \"ladZakina\", \"ladZakI\", \"ladZakina\",\n",
    "    \"mAI\", \"mAI/mAyI\", \"mAina/mAyana\", \"mAI/mAyI\", \"mAina/mAyana\",\n",
    "    \"rAwa\", \"rAwa\", \"rAwa\", \"rAwa\", \"rAwana\",\n",
    "    \"rAwi\", \"rAwi\", \"rAwi\", \"rAwi\", \"rAwina\",\n",
    "    \"Orawa\", \"Orawa\", \"Orawana\", \"Orawa\", \"Orawana\",\n",
    "    \"BAsA\", \"BAsA\", \"BAsA\", \"BAsA\", \"BAsana\",\n",
    "    \"gudZiyA\", \"gudZiyA/gudZiA\", \"gudZiyA/gudZiA\", \"gudZiyA/gudZiA\", \"gudZiyana/gudZiana\",\n",
    "    \"sakwi\", \"sakwi\", \"sakwi\", \"sakwi\", \"sakwina\",\n",
    "    \"qwu\", \"qwu\", \"qwu\", \"qwu\", \"qwuna\",\n",
    "    \"bahU\", \"bahU\", \"bahuna\", \"bahU\", \"bahuna\",\n",
    "    \"lO\", \"lO\", \"lOana\", \"lO\", \"lOana\",\n",
    "    \"mAz\", \"mAz/mAM\", \"mAzvana/mAzyana/mAzina/mAMvana/mAMyana/mAMzina\", \"mAz/mAM\", \"mAzvana/mAzyana/mAzina/mAMvana/mAMyana/mAMzina\",\n",
    "    \"Poto\", \"Poto\", \"Poto\", \"Poto\", \"Potona\",\n",
    "    \"PotoM\", \"PotoM/Potoz\", \"PotoM/Potoz\", \"PotoM/Potoz\", \"PotoMna/Potozna\",\n",
    "    \"BUiM\", \"BUiM/BUiz/BUIM/BUIz\", \"BUiMana/BUizana/BUIMna/BUIzna\", \"BUiM/BUiz/BUIM/BUIz\", \"BUiMana/BUizana/BUIMna/BUIzna\",\n",
    "    \"BOM\", \"BOM/BOz\", \"BOMana/BOzana/Bazuana/BaMuana\", \"BOM/BOz\", \"BOMana/BOzana/Bazuana/BaMuana\",\n",
    "    \"BarasAIM\", \"BarasAIM/BarasAIz\", \"BarasAiMna/barasAizna\", \"BarasAIM/BarasAIz\", \"BarasAiMna/barasAizna\",\n",
    "    \"Bora\", \"Bora\", \"Bora\", \"Bora\", \"Borana\"\n",
    " \n",
    "\n",
    "# Prepare the output structure\n",
    "categories =   \n",
    "word_roots =   \n",
    "word_forms =   \n",
    "\n",
    "# Process the input data\n",
    "for i in range(0, len(data), 5):\n",
    "    word_root = data i \n",
    "    word_form1 = data i + 1 \n",
    "    word_form2 = data i + 2 \n",
    "    word_form3 = data i + 3 \n",
    "    word_form4 = data i + 4 \n",
    "    \n",
    "    categories.append(\"Noun_m_e1\")  # New category for this dataset\n",
    "    word_roots.append(word_root)\n",
    "    word_forms.append( word_form1, word_form2, word_form3, word_form4 )\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Category': categories,\n",
    "    'Word Root': word_roots,\n",
    "    'Word Form 1':  forms 0  for forms in word_forms ,\n",
    "    'Word Form 2':  forms 1  for forms in word_forms ,\n",
    "    'Word Form 3':  forms 2  for forms in word_forms ,\n",
    "    'Word Form 4':  forms 3  for forms in word_forms ,\n",
    "})\n",
    "\n",
    "# Save to Excel\n",
    "df.to_excel('word_data_noun_f.xlsx', index=False)\n",
    "\n",
    "print(\"Data has been saved to 'word_data_noun_m_e1.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e10f063-5542-481c-9fe4-b6b37844b0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to 'word_data_noun_f_e.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Input data\n",
    "data = [\n",
    "    'ladZakI', 'ladZakie', 'ladZakiai', 'ladZakiE', 'ladZakine', 'ladZakinai', 'ladZakinE',\n",
    "    'mAI', 'mAie', 'maie', 'maiai', 'maiE', 'mAiE', 'mAiai', 'mAyie', 'mAyiai', 'mAyE', 'mAine', 'mAinai', 'mAinE',\n",
    "    'rAwa', 'rAwe', 'rAwai', 'rAwE', 'rAwane', 'rAwanai', 'rAwanE',\n",
    "    'rAwi', 'rAwie', 'rAwie', 'rAwiai', 'rAwine', 'rAwinai', 'rawine',\n",
    "    'Orawa', 'Orawe', 'Orawai', 'OrawE', 'Orawane', 'Orawanai', 'OrawanE',\n",
    "    'BAsA', 'BAse', 'BAsai', 'BAsE', 'BAsane', 'BAsanai', 'BAsanE',\n",
    "    'gudZiyA', 'gudZiye', 'gudZie', 'gudZiyai', 'gudZiai', 'gudZiE', 'gudZiyE', 'gudZiyanai', 'gudZiyane', 'gudZianai', 'gudZiane', 'gudZiyanE', 'gudZianE',\n",
    "    'sakwi', 'sakwie', 'sakwiai', 'sakwiE', 'sakwine', 'sakwinai', 'sakwinaE',\n",
    "    'qwu', 'qwue', 'qwuai', 'qwuE', 'qwune', 'qwuanai', 'qwunE',\n",
    "    'bahU', 'bahue', 'bahuai', 'bahuE', 'bahune', 'bahunai', 'bahunE',\n",
    "    'lO', 'lOe', 'lOai', 'lOE', 'lOne', 'lOnai', 'lOnE',\n",
    "    'mAz', 'mAzai', 'mAzE', 'mAze', 'mAMai', 'mAME', 'mAMe', 'mAzvanai', 'mAzvanE', 'mAzvane', 'mAMvanai', 'mAMvanE', 'mAMvane', 'mAzyanai', 'mAzyanE', 'mAzyane', 'mAMyanai', 'mAMyanE', 'mAMyane', 'mAzine', 'mAzinai', 'mAzinE', 'mAMzine', 'mAMzinai', 'mAMzinE',\n",
    "    'Poto', 'Potoe', 'Potoai', 'PotoE', 'Potone', 'Potonai', 'PotonE',\n",
    "    'PotoM', 'Potoze', 'Potazai', 'PotozE', 'PotoMe', 'PotoMai', 'PotoME', 'Potozne', 'Potoznai', 'PotoznE', 'PotoMne', 'PotoMnai', 'PotoMnE',\n",
    "    'BUiM', 'BUiMe', 'BUize', 'BUiMai', 'BUizai', 'BUiME', 'BUizE', 'BUiMane', 'BUizane', 'BUiManai', 'BUizanai', 'BUiManE', 'BUizanE',\n",
    "    'BOM', 'BOMe', 'BOze', 'BOzue', 'BOMue', 'BOME', 'BOzE', 'BOzuE', 'BOMuE', 'BOMai', 'BOzai', 'BOzuai', 'BOMuai', 'BOMane', 'BOzane', 'Bazuane', 'BaMuane', 'BOManai', 'BOzanai', 'Bazuanai', 'BaMuanai', 'BOManE', 'BOzanE', 'BazuanE', 'BaMuanE',\n",
    "    'BarasAIM', 'BarasAiMeM', 'BarasAizez', 'BarasaiMaiM', 'Barasaizaiz', 'BarasAiMEM', 'BarasAizEz', 'BarasAiMne', 'BarasAizne', 'BarasAiMnaiM', 'BarasAiznaiz', 'BarasaiMne', 'Barasaizne', 'BarasaiMnaiM', 'Barasaiznaiz',\n",
    "    'Bora', 'Bore', 'Borai', 'BorE', 'Borane', 'Boranai', 'BoranE'\n",
    "]\n",
    "\n",
    "# Prepare the output structure\n",
    "categories = []\n",
    "word_roots = []\n",
    "word_forms = []\n",
    "\n",
    "# Process the input data\n",
    "for i in range(0, len(data), 7):\n",
    "    word_root = data[i]\n",
    "    forms = data[i+1:i+7]\n",
    "    \n",
    "    categories.append(\"Noun_f_e\")  # New category for this dataset\n",
    "    word_roots.append(word_root)\n",
    "    word_forms.append(forms + [''] * (4 - len(forms)))  # Pad with empty strings to ensure 4 columns\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Category': categories,\n",
    "    'Word Root': word_roots,\n",
    "    'Word Form 1': [forms[0] for forms in word_forms],\n",
    "    'Word Form 2': [forms[1] for forms in word_forms],\n",
    "    'Word Form 3': [forms[2] for forms in word_forms],\n",
    "    'Word Form 4': [forms[3] for forms in word_forms],\n",
    "})\n",
    "\n",
    "# Save to Excel\n",
    "df.to_excel('word_data_noun_f_e.xlsx', index=False)\n",
    "\n",
    "print(\"Data has been saved to 'word_data_noun_f_e.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b7ee1fc-293c-4955-8393-7683d479e2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to 'word_data_noun_f_e1.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Input data\n",
    "data = [\n",
    "    ('ladZakI', 'ladZakio/ladZakiau/ladZakiO', 'ladZakino/ladZakinau/ladZakinO', 'ladZakio/ladZakiau/ladZakiO', 'ladZakino/ladZakinau/ladZakinO'),\n",
    "    ('mAI', 'mAio/mAiau/mAiO/maio/maiau/maiO/mAyio/mAyiau/mAyiO', 'mAino/mAinau/mAinO/maino/mainau/mainO/mAyano/mAyanau/mAyanO', 'mAio/mAiau/mAiO/maio/maiau/maiO/mAyio/mAyiau/mAyiO', 'mAino/mAinau/mAinO/maino/mainau/mainO/mAyano/mAyanau/mAyanO'),\n",
    "    ('rAwa', 'rAwo/rAwO/rAwau', 'rAwo/rAwO/rAwau', 'rAwo/rAwO/rAwau', 'rAwano/rAwanO/rAwanau'),\n",
    "    ('rAwi', 'rAwio/rAwiau/rAwiO', 'rAwio/rAwiau/rAwiO', 'rAwio/rAwiau/rAwiO', 'rAwino/rAwinau/rawinO'),\n",
    "    ('Orawa', 'Orawo/Orawau/OrawO', 'Orawano/Orawanau/OrawanaO/Orawino', 'Orawo/Orawau/OrawO', 'Orawano/Orawanau/OrawanaO/Orawino'),\n",
    "    ('BAsA', 'BAso/BAsO/BAsau/Baso/Basau/BasO', 'BAso/BAsO/BAsau/Baso/Basau/BasO', 'BAso/BAsO/BAsau/Baso/Basau/BasO', 'BAsano/BAsanO/BAsanau/Basano/Basanau/BasanO'),\n",
    "    ('gudZiyA', 'gudZiyo/gudZiyO/gudZiyau/gudZio/gudZiau/gudZiO', 'gudZiyo/gudZiyO/gudZiyau/gudZio/gudZiau/gudZiO', 'gudZiyo/gudZiyO/gudZiyau/gudZio/gudZiau/gudZiO', 'gudZiyano/gudZiyanO/gudZiyanau/gudZiano/gudZianO/gudZianau'),\n",
    "    ('sakwi', 'sakwio/sakwiau/sakwiO', 'sakwio/sakwiau/sakwiO', 'sakwio/sakwiau/sakwiO', 'sakwino/sakwinau/sakwinaO'),\n",
    "    ('qwu', 'qwuo/qwuau/qwuO', 'qwuo/qwuau/qwuO', 'qwuo/qwuau/qwuO', 'qwuno/qwuanau/qwunO'),\n",
    "    ('bahU', 'bahuo/bahuau/bahuO', 'bahuno/bahunau/bahunO', 'bahuo/bahuau/bahuO', 'bahuno/bahunau/bahunO'),\n",
    "    ('lO', 'lOVo/lOVO/lOau', 'lOno/lOnO/lOnau', 'lOVo/lOVO/lOau', 'lOno/lOnO/lOnau'),\n",
    "    ('mAz', 'mAzau/mAzO/mAzo/mAMau/mAMO/mAMo', 'mAzvanau/mAzvanO/mAzvano/mAMvanau/mAMvanO/mAMvano/mAzyanau/mAzyanO/mAzyano/mAMyanau/mAMyanO/mAMyano/mAzino/mAzinau/mAzinO/mAMzino/mAMzinau/mAMzinO', 'mAzau/mAzO/mAzo/mAMau/mAMO/mAMo', 'mAzvanau/mAzvanO/mAzvano/mAMvanau/mAMvanO/mAMvano/mAzyanau/mAzyanO/mAzyano/mAMyanau/mAMyanO/mAMyano/mAzino/mAzinau/mAzinO/mAMzino/mAMzinau/mAMzinO'),\n",
    "    ('Poto', 'Potoo/Potoau/PotoO', 'Potoo/Potoau/PotoO', 'Potoo/Potoau/PotoO', 'Potono/Potanau/PotanO'),\n",
    "    ('PotoM', 'PotoMo/Potozo/PotoMau/Potazau/PotoMO/PotozO', 'PotoMo/Potozo/PotoMau/Potazau/PotoMO/PotozO', 'PotoMo/Potozo/PotoMau/Potazau/PotoMO/PotozO', 'PotoMno/Potozno/Potoznau/PotoMnau/PotoMnO/PotoznO'),\n",
    "    ('BUiM', 'BUiMo/BUizo/BUiMau/BUizau/BUiMO/BUizO', 'BUiMano/BUizano/BUiManau/BUizanau/BUiManO/BUizanO', 'BUiMo/BUizo/BUiMau/BUizau/BUiMO/BUizO', 'BUiMano/BUizano/BUiManau/BUizanau/BUiManO/BUizanO'),\n",
    "    ('BOM', 'BOMo/BOzo/BOzuo/BOMuo/BOMO/BOzO/BOzuO/BOMuO/BOMau/BOzau/BOzuau/BOMuau', 'BOMano/BOzano/Bazuano/BaMuano/BOManau/BOzanau/Bazuanau/BaMuanau/BOManO/BOzanO/BazuanO/BaMuanO', 'BOMo/BOzo/BOzuo/BOMuo/BOMO/BOzO/BOzuO/BOMuO/BOMau/BOzau/BOzuau/BOMuau', 'BOMano/BOzano/Bazuano/BaMuano/BOManau/BOzanau/Bazuanau/BaMuanau/BOManO/BOzanO/BazuanO/BaMuanO'),\n",
    "    ('BarasAIM', 'BarasAiMoM/BarasAizoz/BarasaiMauM/Barasaizauz/BarasAiMOM/BarasAizOz', 'BarasAiMno/BarasAizno/BarasAiMnauM/BarasAiznauz/BarasaiMno/Barasaizno/BarasaiMnauM/Barasaiznauz', 'BarasAiMoM/BarasAizoz/BarasaiMauM/Barasaizauz/BarasAiMOM/BarasAizOz', 'BarasAiMno/BarasAizno/BarasAiMnauM/BarasAiznauz/BarasaiMno/Barasaizno/BarasaiMnauM/Barasaiznauz'),\n",
    "    ('Bora', 'Boro/BorO/Borau', 'Boro/BorO/Borau', 'Boro/BorO/Borau', 'Borano/BoranO/Boranau')\n",
    "]\n",
    "\n",
    "# Prepare the output structure\n",
    "categories = []\n",
    "word_roots = []\n",
    "word_forms = []\n",
    "\n",
    "# Process the input data\n",
    "for entry in data:\n",
    "    word_root, form1, form2, form3, form4 = entry\n",
    "    categories.append(\"Noun_f_e1\")  # New category for this dataset\n",
    "    word_roots.append(word_root)\n",
    "    word_forms.append([form1, form2, form3, form4])\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Category': categories,\n",
    "    'Word Root': word_roots,\n",
    "    'Word Form 1': [forms[0] for forms in word_forms],\n",
    "    'Word Form 2': [forms[1] for forms in word_forms],\n",
    "    'Word Form 3': [forms[2] for forms in word_forms],\n",
    "    'Word Form 4': [forms[3] for forms in word_forms],\n",
    "})\n",
    "\n",
    "# Save to Excel\n",
    "df.to_excel('word_data_noun_f_e1.xlsx', index=False)\n",
    "\n",
    "print(\"Data has been saved to 'word_data_noun_f_e1.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcbaca63-ec2c-43a8-b81d-fca49816deee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to 'word_data_noun_f_rednt.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Input data\n",
    "data = [\n",
    "    ('ladZakiyA', 'ladZakiyA', 'ladZakiyana/ladZakiana', 'ladZakiyA', 'ladZakiyana/ladZakiana'),\n",
    "    ('maiyA', 'maiyA/maiA', 'maiyana/maiana', 'maiyA/maiA', 'maiyana/maiana'),\n",
    "    ('rawiyA', 'rawiyA/rawiA', 'rawiyA/rawiA', 'rawiyA/rawiA', 'rawiyana/rawiana'),\n",
    "    ('OrawiyA', 'OrawiyA/OrawiA', 'Orawiyana/Orawiana', 'OrawiyA/OrawiA', 'Orawiyana/Orawiana'),\n",
    "    ('BasavA', 'BasavA', 'BasavA', 'BasavA', 'Basavana'),\n",
    "    ('lauvA', 'lauvA/lauA', 'lauvana/lauana', 'lauvA/lauA', 'lauvana/lauana'),\n",
    "    ('gudZiyavA', 'gudZiyavA/gudZiavA', 'gudZiyavA/gudZiavA', 'gudZiyavA/gudZiavA', 'gudZiyavana/gudZiavana'),\n",
    "    ('sakwiyA', 'sakwiyA/sakwiA', 'sakwiyA/sakwiA', 'sakwiyA/sakwiA', 'sakwiyana/sakwiana'),\n",
    "    ('qwuvA', 'qwuvA/qwuA', 'qwuvA/qwuA', 'qwuvA/qwuA', 'qwuvana/qwuana'),\n",
    "    ('bahuvA', 'bahuvA/bahuA', 'bahuvana/bahuana', 'bahuvA/bahuA', 'bahuvana/bahuana'),\n",
    "    ('BoravA', 'BoravA', 'BoravA', 'BoravA', 'Boravana'),\n",
    "    ('BarasaiMyAM', 'BarasaiMyAM/BarasaiMyAz', 'BarasaiMyana/Barasaizyana', 'BarasaiMyAM/BarasaiMyAz', 'BarasaiMyana/Barasaizyana/BarasaiMana/Barasaizana'),\n",
    "    ('BOMvAM', 'BOMvAM/BOzvAz', 'BOMvana/BOzvana', 'BOMvAM/BOzvAz', 'BOMvana/BOzvana'),\n",
    "    ('PotoMiyAM', 'PotoMiyAM/PotoMiyAz', 'PotoMiyAM/PotoMiyAz', 'PotoMiyAM/PotoMiyAz', 'PotoMiyana/Potoziyana/PotoMiana/Potoziana'),\n",
    "    ('BuiMyAM', 'BuiMyAM/BuiMyAz/BuizyAM/BuizyAz', 'BuiMyana/Buizyana/BuiMana/Buizana', 'BuiMyAM/BuiMyAz/BuizyAM/BuizyAz', 'BuiMyana/Buizyana/BuiMana/Buizana'),\n",
    "    ('PotoiyA', 'PotoiyA', 'PotoiyA', 'PotoiyA', 'Potoiyana/Potoiana')\n",
    "]\n",
    "\n",
    "# Prepare the output structure\n",
    "categories = []\n",
    "word_roots = []\n",
    "word_forms = []\n",
    "\n",
    "# Process the input data\n",
    "for entry in data:\n",
    "    word_root, form1, form2, form3, form4 = entry\n",
    "    categories.append(\"Noun_f_rednt\")  # New category for this dataset\n",
    "    word_roots.append(word_root)\n",
    "    word_forms.append([form1, form2, form3, form4])\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Category': categories,\n",
    "    'Word Root': word_roots,\n",
    "    'Word Form 1': [forms[0] for forms in word_forms],\n",
    "    'Word Form 2': [forms[1] for forms in word_forms],\n",
    "    'Word Form 3': [forms[2] for forms in word_forms],\n",
    "    'Word Form 4': [forms[3] for forms in word_forms],\n",
    "})\n",
    "\n",
    "# Save to Excel\n",
    "df.to_excel('word_data_noun_f_rednt.xlsx', index=False)\n",
    "\n",
    "print(\"Data has been saved to 'word_data_noun_f_rednt.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3166acfe-7a7f-45bb-9d27-f551fd136cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to 'word_data_noun_f_rednt_e.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Input data\n",
    "data = [\n",
    "    ('ladZakiyA', 'ladZakiye/ladZakiyai/ladZakiyE', 'ladZakiyanai/ladZakiyane/ladZakiyanE/ladZakianai/ladZakiane/ladZakianE', 'ladZakiye/ladZakiyai/ladZakiyE', 'ladZakiyanai/ladZakiyane/ladZakiyanE/ladZakianai/ladZakiane/ladZakianE'),\n",
    "    ('maiyA', 'maiye/maiyai/maiyE/maie/maiai/maiE', 'maiyanai/maiyane/maiyanE/maianai/maiane/maianE', 'maiye/maiyai/maiyE/maie/maiai/maiE', 'maiyanai/maiyane/maiyanE/maianai/maiane/maianE'),\n",
    "    ('rawiyA', 'rawiye/rawiyai/rawiyE/rawie/rawiai/rawiE', 'rawiye/rawiyai/rawiyE/rawie/rawiai/rawiE', 'rawiye/rawiyai/rawiyE/rawie/rawiai/rawiE', 'rawiyanai/rawiyane/rawiyanE/rawiane/rawianai/rawianE'),\n",
    "    ('OrawiyA', 'Orawiye/Orawiyai/OrawiyE/Orawie/Orawiai/OrawiE', 'Orawiyanai/Orawiyane/OrawiyanE/Orawiane/Orawianai/OrawianE', 'Orawiye/Orawiyai/OrawiyE/Orawie/Orawiai/OrawiE', 'Orawiyanai/Orawiyane/OrawiyanE/Orawiane/Orawianai/OrawianE'),\n",
    "    ('BasavA', 'Basave/Basavai/BasavE', 'Basave/Basavai/BasavE', 'Basave/Basavai/BasavE', 'Basavanai/Basavane/BasavanE'),\n",
    "    ('lauvA', 'lauve/lauvai/lauvE/laue/lauai/lauE', 'lauvane/lauvanE/lauvanai/lauane/lauanai/laquanE', 'lauve/lauvai/lauvE/laue/lauai/lauE', 'lauvane/lauvanE/lauvanai/lauane/lauanai/laquanE'),\n",
    "    ('gudZiyavA', 'gudZiyavai/gudZiyave/gudZiyavE/gudZiavai/gudZiave/gudZiavE', 'gudZiyavai/gudZiyave/gudZiyavE/gudZiavai/gudZiave/gudZiavE', 'gudZiyavai/gudZiyave/gudZiyavE/gudZiavai/gudZiave/gudZiavE', 'gudZiyavanai/gudZiyavane/gudZiyavanE/gudZiavane/gudZiavanai/gudZiavanE'),\n",
    "    ('sakwiyA', 'sakwiye/sakwiyai/sakwiyE', 'sakwiye/sakwiyai/sakwiyE', 'sakwiye/sakwiyai/sakwiyE', 'sakwiyanai/sakwiyane/sakwiyanE'),\n",
    "    ('bahuvA', 'bahuve/bahuvai/bahuvE', 'bahuve/bahuvai/bahuvE', 'bahuve/bahuvai/bahuvE', 'bahuvanai/bahuvane/bahuvanE'),\n",
    "    ('qwuvA', 'qwuve/qwuvE/qwuvai', 'qwuve/qwuvE/qwuvai', 'qwuve/qwuvE/qwuvai', 'qwuvane/qwuvanE/qwuvnai'),\n",
    "    ('BoravA', 'Borave/Boravai/BoravE', 'Borave/Boravai/BoravE', 'Borave/Boravai/BoravE', 'Boravanai/Boravane/BoravanE'),\n",
    "    ('BarasaiMyAM', 'BarasaiMyeM/BarasaiMyEM/BarasaiMyaiM/BarasaiMyez/BarasaiMyEz/BarasaiMyaiz/BarasaizyeM/BarasaizyEM/BarasaizyaiM/Barasaizyez/BarasaizyEz/Barasaizyaiz', 'BarasaiMyaneM/BarasaiMyanaiM/BarasaiMyanEM/BarasaiMyanez/BarasaiMyanaiz/BarasaiMyanEz/Barasaizyanez/Barasaizyanaiz/BarasaizyanEz/BarasaizyaneM/BarasaizyanaiM/BarasaizyanEM', 'BarasaiMyeM/BarasaiMyEM/BarasaiMyaiM/BarasaiMyez/BarasaiMyEz/BarasaiMyaiz/BarasaizyeM/BarasaizyEM/BarasaizyaiM/Barasaizyez/BarasaizyEz/Barasaizyaiz', 'BarasaiMyaneM/BarasaiMyanaiM/BarasaiMyanEM/BarasaiMyanez/BarasaiMyanaiz/BarasaiMyanEz/Barasaizyanez/Barasaizyanaiz/BarasaizyanEz/BarasaizyaneM/BarasaizyanaiM/BarasaizyanEM'),\n",
    "    ('BOMvAM', 'BOMveM/BOzvez/BOMvaiM/BOzvaiz/BOMvEM/BOzvEz/BOMvez/BOzveM/BOMvaiz/BOzvaiM/BOMvEz/BOzvEM', 'BOMvane/BOMvanai/BOMvanai/Bozvane/Bozvanai/BozvanE', 'BOMveM/BOzvez/BOMvaiM/BOzvaiz/BOMvEM/BOzvEz/BOMvez/BOzveM/BOMvaiz/BOzvaiM/BOMvEz/BOzvEM', 'BOMvane/Bozvane/BOMvanai/BOzvanai/BOMvanE/BozvanEe/'),\n",
    "    ('PotoM', 'PotoMiyeM/Potoziyez/PotoMiyez/PotoziyeM/PotoMiyEM/PotoziyEz/PotoMiyEz/PotoziyEM/PotoMiyaiM/Potoziyaiz/PotoMiyaiz/PotoziyaiM', 'PotoMiyeM/Potoziyez/PotoMiyez/PotoziyeM/PotoMiyEM/PotoziyEz/PotoMiyEz/PotoziyEM/PotoMiyaiM/Potoziyaiz/PotoMiyaiz/PotoziyaiM', 'PotoMiyeM/Potoziyez/PotoMiyez/PotoziyeM/PotoMiyEM/PotoziyEz/PotoMiyEz/PotoziyEM/PotoMiyaiM/Potoziyaiz/PotoMiyaiz/PotoziyaiM', 'PotoMiyane/Potoziyane/PotoMiyanai/Potoziyanai/PotoMiyanE/PotoziyanE'),\n",
    "    ('BuiMyAM', 'BuiMyeM/Buizyez/BuiMyez/BuizyeM/BuiMyaiM/Buizyaiz/BuiMyaiz/BuizyaiM/BuiMyEM/BuizyEz/BuiMyEz/BuizyEM', 'BuiMyanai/Buizyanai/BuiMyane/Buizyane/BuiMyanE/BuizyanE', 'BuiMyeM/Buizyez/BuiMyez/BuizyeM/BuiMyaiM/Buizyaiz/BuiMyaiz/BuizyaiM/BuiMyEM/BuizyEz/BuiMyEz/BuizyEM', 'BuiMyanai/Buizyanai/BuiMyane/Buizyane/BuiMyanE/BuizyanE'),\n",
    "    ('PotoiyA', 'Potoiye/Potoiyai/PotoiE', 'Potoiye/Potoiyai/PotoiE', 'Potoiye/Potoiyai/PotoiE', 'Potoiyane/Potyanai/PotanE')\n",
    "]\n",
    "\n",
    "# Prepare the output structure\n",
    "categories = []\n",
    "word_roots = []\n",
    "word_forms = []\n",
    "\n",
    "# Process the input data\n",
    "for entry in data:\n",
    "    word_root, form1, form2, form3, form4 = entry\n",
    "    categories.append(\"Noun_f_rednt_e\")  # New category for this dataset\n",
    "    word_roots.append(word_root)\n",
    "    word_forms.append([form1, form2, form3, form4])\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Category': categories,\n",
    "    'Word Root': word_roots,\n",
    "    'Word Form 1': [forms[0] for forms in word_forms],\n",
    "    'Word Form 2': [forms[1] for forms in word_forms],\n",
    "    'Word Form 3': [forms[2] for forms in word_forms],\n",
    "    'Word Form 4': [forms[3] for forms in word_forms],\n",
    "})\n",
    "\n",
    "# Save to Excel\n",
    "df.to_excel('word_data_noun_f_rednt_e.xlsx', index=False)\n",
    "\n",
    "print(\"Data has been saved to 'word_data_noun_f_rednt_e.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5f6730a-6ba7-4e00-b0ea-2708bb3625f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to 'word_data_noun_f_rednt_e1.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Input data\n",
    "data = [\n",
    "    ('ladZakiyA', 'ladZakiyo/ladZakiyO/ladZakiyau', 'ladZakiyano/ladZakiyanO/ladZakiyanau/ladZakiano/ladZakianO/ladZakianau', 'ladZakiyo/ladZakiyO/ladZakiyau', 'ladZakiyano/ladZakiyanO/ladZakiyanau/ladZakiano/ladZakianO/ladZakianau'),\n",
    "    ('maiyA', 'maiyo/maiyO/maiyau/maio/maiau/maiO', 'maiyano/maiyanO/maiyanau/maiano/maianO/maianau', 'maiyo/maiyO/maiyau/maio/maiau/maiO', 'maiyano/maiyanO/maiyanau/maiano/maianO/maianau'),\n",
    "    ('rawiyA', 'rawiyo/rawiyO/rawiyau/rawio/rawiau/rawiO', 'rawiyo/rawiyO/rawiyau/rawio/rawiau/rawiO', 'rawiyo/rawiyO/rawiyau/rawio/rawiau/rawiO', 'rawiyano/rawiyanO/rawiyanau/rawiano/rawianO/rawianau'),\n",
    "    ('OrawiyA', 'Orawiyo/OrawiyO/Orawiyau/Orawio/OrawiO/Orawiau', 'Orawiyano/OrawiyanO/Orawiyanau/Orawiano/OrawianO/Orawianau', 'Orawiyo/OrawiyO/Orawiyau/Orawio/OrawiO/Orawiau', 'Orawiyano/OrawiyanO/Orawiyanau/Orawiano/OrawianO/Orawianau'),\n",
    "    ('BasavA', 'Basavo/BasavO/Basavau', 'Basavo/BasavO/Basavau', 'Basavo/BasavO/Basavau', 'Basavano/BasavanO/Basavanau'),\n",
    "    ('lauvA', 'lauvo/lauvau/lauvO/lauo/lauau/lauO', 'lauvano/lauano/lauvanau/lauanau/lauvanO/lauanO/', 'lauvo/lauvau/lauvO/lauo/lauau/lauO', 'lauvano/lauano/lauvanau/lauanau/lauvanO/lauanO/'),\n",
    "    ('gudZiyavA', 'gudZiyavo/gudZiavo/gudZiyavau/gudZiavau/gudZiyavO/gudZiavO', 'gudZiyavo/gudZiavo/gudZiyavau/gudZiavau/gudZiyavO/gudZiavO', 'gudZiyavo/gudZiavo/gudZiyavau/gudZiavau/gudZiyavO/gudZiavO', 'gudZiyavano/gudZiyavanO/gudZiyavanau/gudZiavano/gudZiavanau/gudZiavanO'),\n",
    "    ('sakwiyA', 'sakwiyo/sakwiyO/sakwiyau', 'sakwiyo/sakwiyO/sakwiyau', 'sakwiyo/sakwiyO/sakwiyau', 'sakwiyano/sakwiyanO/sakwiyanau'),\n",
    "    ('bahuvA', 'bahuvo/bahuvO/bahuvau', 'bahuvo/bahuvO/bahuvau', 'bahuvo/bahuvO/bahuvau', 'bahuvano/bahuvanO/bahuvanau'),\n",
    "    ('qwuvA', 'qwuvo/qwuvO/qwuvau', 'qwuvo/qwuvO/qwuvau', 'qwuvo/qwuvO/qwuvau', 'qwuvano/qwuvanO/qwuvanau'),\n",
    "    ('BoravA', 'Boravo/BoravO/Boravau', 'Boravo/BoravO/Boravau', 'Boravo/BoravO/Boravau', 'Boravano/BoravanO/Boravanau'),\n",
    "    ('BarasaiMyAM', 'BarasaiMyoM/BarasaiMyOM/BarasaiMyauM/BarasaiMyoz/BarasaiMyOz/BarasaiMyauz/BarasaizyoM/BarasaizyOM/BarasaizyauM/Barasaizyoz/BarasaizyOz/Barasaizyauz', 'BarasaiMyanoM/BarasaiMyanauM/BarasaiMyanOM/BarasaiMyanoz/BarasaiMyanauz/BarasaiMyanOz/Barasaizyanoz/Barasaizyanauz/BarasaizyanOz/BarasaizyanoM/BarasaizyanauM/BarasaizyanOM', 'BarasaiMyoM/BarasaiMyOM/BarasaiMyauM/BarasaiMyoz/BarasaiMyOz/BarasaiMyauz/BarasaizyoM/BarasaizyOM/BarasaizyauM/Barasaizyoz/BarasaizyOz/Barasaizyauz', 'BarasaiMyanoM/BarasaiMyanauM/BarasaiMyanOM/BarasaiMyanoz/BarasaiMyanauz/BarasaiMyanOz/Barasaizyanoz/Barasaizyanauz/BarasaizyanOz/BarasaizyanoM/BarasaizyanauM/BarasaizyanOM'),\n",
    "    ('BOMvAM', 'BOMvoM/BOzvoz/BOMvauM/BOzvauz/BOMvOM/BOzvOz/BOMvoz/BOzvoM/BOMvauz/BOzvaM/BOMvOz/BOzvOM', 'BOMvano/Bozvano/BOMvanau/Bozvanau/BOMvanO/BozvanO', 'BOMvoM/BOzvoz/BOMvauM/BOzvauz/BOMvOM/BOzvOz/BOMvoz/BOzvoM/BOMvauz/BOzvaM/BOMvOz/BOzvOM', 'BOMvano/Bozvano/BOMvanau/Bozvanau/BOMvanO/BozvanO'),\n",
    "    ('PotoM', 'PotoMiyoM/Potoziyoz/PotoMiyoz/PotoziyoM/PotoMiyOM/PotoziyOz/PotoMiyOz/PotoziyOM/PotoMiyauM/Potoziyauz/PotoMiyauz/PotoziyauM', 'PotoMiyoM/Potoziyoz/PotoMiyoz/PotoziyoM/PotoMiyOM/PotoziyOz/PotoMiyOz/PotoziyOM/PotoMiyauM/Potoziyauz/PotoMiyauz/PotoziyauM', 'PotoMiyoM/Potoziyoz/PotoMiyoz/PotoziyoM/PotoMiyOM/PotoziyOz/PotoMiyOz/PotoziyOM/PotoMiyauM/Potoziyauz/PotoMiyauz/PotoziyauM', 'PotoMiyano/Potoziyano/PotoMiyanO/PotoziyanO/PotoMiyanau/Potoziyanau'),\n",
    "    ('BuiMyAM', 'BuiMyoM/Buizyoz/BuiMyoz/BuizyoM/BuiMyauM/Buizyauz/BuiMyauz/BuizyauM/BuiMyOM/BuizyOz/BuiMyOz/BuizyOM', 'BuiMyanau/Buizyanau/BuiMyanO/BuizyanO/BuiMyano/Buizyano', 'BuiMyoM/Buizyoz/BuiMyoz/BuizyoM/BuiMyauM/Buizyauz/BuiMyauz/BuizyauM/BuiMyOM/BuizyOz/BuiMyOz/BuizyOM', 'BuiMyanau/Buizyanau/BuiMyanO/BuizyanO/BuiMyano/Buizyano'),\n",
    "    ('PotoiyA', 'Potoiyo/Potoiyau/PotoiO', 'Potoiyo/Potoiyau/PotoiO', 'Potoiyo/Potoiyau/PotoiO', 'Potoiyano/Potyanau/PotanO')\n",
    "]\n",
    "# Prepare the output structure\n",
    "categories = []\n",
    "word_roots = []\n",
    "word_forms = []\n",
    "\n",
    "# Process the input data\n",
    "for entry in data:\n",
    "    word_root, form1, form2, form3, form4 = entry\n",
    "    categories.append(\"Noun_f_rednt_e1\")  # New category for this dataset\n",
    "    word_roots.append(word_root)\n",
    "    word_forms.append([form1, form2, form3, form4])\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Category': categories,\n",
    "    'Word Root': word_roots,\n",
    "    'Word Form 1': [forms[0] for forms in word_forms],\n",
    "    'Word Form 2': [forms[1] for forms in word_forms],\n",
    "    'Word Form 3': [forms[2] for forms in word_forms],\n",
    "    'Word Form 4': [forms[3] for forms in word_forms],\n",
    "})\n",
    "\n",
    "# Save to Excel\n",
    "df.to_excel('word_data_noun_f_rednt_e1.xlsx', index=False)\n",
    "\n",
    "print(\"Data has been saved to 'word_data_noun_f_rednt_e1.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b23c49e-a705-4847-95cd-42ad7febce24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to 'Advrb_e_dataset.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Input data for Advrb_e\n",
    "data = [\n",
    "    ('aba', 'abbai/abai/aBI/abahi/abahI'),\n",
    "    ('kala', 'kalai/kAlhai/kalE/kAlhE'),\n",
    "    ('Aja', 'Ajai/AjE/Ajahi/Ajuve'),\n",
    "    ('parasoM', 'parasavaiz/parasaveM/parasavahi/parasave/parasoMveM/parasoveM/parasoMyeM/parasoMeM/parasozvez/parasovez/parasozyeM/parasoMez'),\n",
    "    ('bihAna', 'bihAnai/bihAne/bihAnE/bihanai'),\n",
    "    ('sabere', 'sabarahi/sabarahI/saberahi/saberahI/saberai'),\n",
    "    ('subaha', 'subahai/subahiyai/subahe'),\n",
    "    ('XIre', 'XirahI/Xirahi/XirehI/XIrehi/XirahIM/XirahiM'),\n",
    "    ('hamesA', 'hamesahi/hamesahI/hamesai'),\n",
    "    ('nIce', 'nIcahi/nIcahI/nIcahai/nicehI/nicehi/nIcahE'),\n",
    "    ('aso', 'asavai/asave/asavahi/asave/asove/asoye/asoe'),\n",
    "    ('begahUz', 'begahuzveM/begahuMveM/begahuzvez/begahuMvez/begahuzyeM/begahuMyeM/begahuzyez/begahuMyez'),\n",
    "    ('anaxeKe', 'anaxeKalai/anaxeKale'),\n",
    "    ('ahewu', 'ahewue/ahewuve/ahewuye'),\n",
    "    ('beAbarU', 'beAbarue/beAbaruve/beAbaruye'),\n",
    "    ('awi', 'awiye/awie/awiyai/awiai'),\n",
    "    ('kaBI', 'kaBiye/kaBie/kaBiyai'),\n",
    "    ('abahIz', 'abahizye/abahiMye/abahize/abahiMe/abahiMyai/abahizyai'),\n",
    "    ('jahAz', 'jahez/jaheM/jahEz/jahEM')\n",
    "]\n",
    "\n",
    "# Prepare the output structure\n",
    "# Prepare the output structure\n",
    "categories = []\n",
    "word_roots = []\n",
    "word_forms1 = []\n",
    "word_forms2 = []\n",
    "word_forms3 = []\n",
    "word_forms4 = []\n",
    "\n",
    "# Process the input data\n",
    "for entry in data:\n",
    "    word_root, forms = entry\n",
    "    forms_list = forms.split('/')\n",
    "    categories.append(\"Advrb_e\")  # New category for this dataset\n",
    "    word_roots.append(word_root)\n",
    "    \n",
    "    # Ensure we handle up to 4 forms, pad with empty strings if fewer forms are present\n",
    "    for i in range(4):\n",
    "        if i < len(forms_list):\n",
    "            if i == 0:\n",
    "                word_forms1.append(forms_list[i])\n",
    "            elif i == 1:\n",
    "                word_forms2.append(forms_list[i])\n",
    "            elif i == 2:\n",
    "                word_forms3.append(forms_list[i])\n",
    "            elif i == 3:\n",
    "                word_forms4.append(forms_list[i])\n",
    "        else:\n",
    "            # Pad missing forms with empty strings\n",
    "            if i == 0:\n",
    "                word_forms1.append('')\n",
    "            elif i == 1:\n",
    "                word_forms2.append('')\n",
    "            elif i == 2:\n",
    "                word_forms3.append('')\n",
    "            elif i == 3:\n",
    "                word_forms4.append('')\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Category': categories,\n",
    "    'Word Root': word_roots,\n",
    "    'Word Form 1': word_forms1,\n",
    "    'Word Form 2': word_forms2,\n",
    "    'Word Form 3': word_forms3,\n",
    "    'Word Form 4': word_forms4\n",
    "})\n",
    "\n",
    "# Save to an Excel file\n",
    "df.to_excel('Advrb_e_dataset3.xlsx', index=False)\n",
    "\n",
    "print(\"Data has been saved to 'Advrb_e_dataset.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61265e19-b2d1-46ca-bf50-588a602195fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from docx import Document\n",
    "\n",
    "# Load the Word document\n",
    "doc_path = 'B_M_M_Word-generation-ver-1.9.0.docx'\n",
    "document = Document(doc_path)\n",
    "\n",
    "# Initialize lists to store data from the \"Category\" and \"SL inputs\"\n",
    "categories = []\n",
    "sl_inputs = []\n",
    "\n",
    "# Iterate through the tables in the document\n",
    "for table in document.tables:\n",
    "    for row in table.rows:\n",
    "        # Assuming \"Category\" is in the first column (index 0) \n",
    "        # and \"SL inputs\" is in the second column (index 1)\n",
    "        categories.append(row.cells[0].text)\n",
    "        sl_inputs.append(row.cells[1].text)\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "data = {\n",
    "    'Category': categories,\n",
    "    'SL Inputs': sl_inputs\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "excel_file_path = 'extracted_categories_sl_inputs.xlsx'\n",
    "df.to_excel(excel_file_path, index=False)\n",
    "\n",
    "print(f\"Data from the 'Category' and 'SL inputs' has been extracted and saved to '{excel_file_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f4a1516-97b8-42cc-b051-2d3641700884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from the 'Category' and 'SL inputs' has been extracted and saved to 'extracted_categories_TL Lexical Transfer (Hindi).xlsx'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from docx import Document\n",
    "\n",
    "# Load the Word document\n",
    "doc_path = 'B_M_M_Word-generation-ver-1.9.0.docx'\n",
    "document = Document(doc_path)\n",
    "\n",
    "# Initialize lists to store data from the \"Category\" and \"SL inputs\"\n",
    "categories = []\n",
    "sl_inputs = []\n",
    "\n",
    "# Iterate through the tables in the document\n",
    "for table in document.tables:\n",
    "    for row in table.rows:\n",
    "        # Ensure the row has enough cells to avoid IndexError\n",
    "        if len(row.cells) >= 3:  # Check if row has at least 3 cells\n",
    "            # Assuming \"Category\" is in the first column (index 0) \n",
    "            # and \"SL inputs\" is in the third column (index 2)\n",
    "            categories.append(row.cells[0].text)\n",
    "            sl_inputs.append(row.cells[2].text)\n",
    "        else:\n",
    "            # Add empty values for rows with fewer than 3 cells\n",
    "            categories.append(row.cells[0].text if len(row.cells) > 0 else \"\")\n",
    "            sl_inputs.append(\"\")\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "data = {\n",
    "    'Category': categories,\n",
    "    'TL Lexical Transfer (Hindi) (Word Generator Input)': sl_inputs\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "excel_file_path = 'extracted_categories_TL Lexical Transfer (Hindi).xlsx'\n",
    "df.to_excel(excel_file_path, index=False)\n",
    "\n",
    "print(f\"Data from the 'Category' and 'SL inputs' has been extracted and saved to '{excel_file_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df60f93a-7c85-45d3-82b9-f7d9c9017982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from the 'Category' and 'SL inputs' has been extracted and saved to 'extracted_categories_Substituted Root for Test Case.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from docx import Document\n",
    "\n",
    "# Load the Word document\n",
    "doc_path = 'B_M_M_Word-generation-ver-1.9.0.docx'\n",
    "document = Document(doc_path)\n",
    "\n",
    "# Initialize lists to store data from the \"Category\" and \"SL inputs\"\n",
    "categories = []\n",
    "sl_inputs = []\n",
    "\n",
    "# Iterate through the tables in the document\n",
    "for table in document.tables:\n",
    "    for row in table.rows:\n",
    "        # Ensure the row has enough cells to avoid IndexError\n",
    "        if len(row.cells) >= 4:  # Check if row has at least 3 cells\n",
    "            # Assuming \"Category\" is in the first column (index 0) \n",
    "            # and \"SL inputs\" is in the third column (index 2)\n",
    "            categories.append(row.cells[0].text)\n",
    "            sl_inputs.append(row.cells[3].text)\n",
    "        else:\n",
    "            # Add empty values for rows with fewer than 3 cells\n",
    "            categories.append(row.cells[0].text if len(row.cells) > 0 else \"\")\n",
    "            sl_inputs.append(\"\")\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "data = {\n",
    "    'Category': categories,\n",
    "    'Substituted Root for Test Case': sl_inputs\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "excel_file_path = 'extracted_categories_Substituted Root for Test Case.xlsx'\n",
    "df.to_excel(excel_file_path, index=False)\n",
    "\n",
    "print(f\"Data from the 'Category' and 'SL inputs' has been extracted and saved to '{excel_file_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "90b27eb7-597c-4ead-9877-a16abb380cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from columns 0, 4, and 5 has been extracted and saved to 'checfk.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from docx import Document\n",
    "\n",
    "# Load the Word document\n",
    "doc_path = 'B_M_M_Word-generation-ver-1.9.0.docx'\n",
    "document = Document(doc_path)\n",
    "\n",
    "# Initialize lists to store data from the specified columns\n",
    "column_0 = []\n",
    "column_4 = []\n",
    "column_5 = []\n",
    "\n",
    "# Iterate through the tables in the document\n",
    "for table in document.tables:\n",
    "    for row in table.rows:\n",
    "        # Ensure the row has enough cells to avoid IndexError\n",
    "        if len(row.cells) >= 6:  # Check if row has at least 6 cells\n",
    "            # Extract data from columns 0, 4, and 5\n",
    "            column_0.append(row.cells[0].text)\n",
    "            column_4.append(row.cells[4].text)\n",
    "            column_5.append(row.cells[5].text)\n",
    "        else:\n",
    "            # Add empty values for rows with fewer than 6 cells\n",
    "            column_0.append(row.cells[0].text if len(row.cells) > 0 else \"\")\n",
    "            column_4.append(row.cells[4].text if len(row.cells) > 4 else \"\")\n",
    "            column_5.append(row.cells[5].text if len(row.cells) > 5 else \"\")\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "data = {\n",
    "    'Category': column_0,\n",
    "    'TL Generation (Hindi)> Rules+Conditions': column_4,\n",
    "    'Gold Output of Test Case': column_5\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "excel_file_path = 'extracted_categories_TL Generation (Hindi)> Rules+Conditions_.xlsx'\n",
    "df.to_excel(excel_file_path, index=False)\n",
    "\n",
    "print(f\"Data from columns 0, 4, and 5 has been extracted and saved to '{excel_file_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ba2cd4b-4022-466c-81ab-7589573ce0e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "479f3a02-bf52-4182-82d2-e71539447f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XML file 'paradigm-bhojpuri-checkpoint.xml' generated successfully.\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Define file paths\n",
    "input_file = 'B_M_M_Word-generation-ver-1.9.0.txt'\n",
    "output_file = 'paradigm-bhojpuri-checkpoint.xml'\n",
    "\n",
    "# Create the root element for the XML\n",
    "root = ET.Element(\"ParadigmData\")\n",
    "\n",
    "# Open and parse the file line by line\n",
    "with open(input_file, 'r', encoding='utf-8') as file:\n",
    "    category = None\n",
    "    category_element = None\n",
    "    fs_element = None\n",
    "    \n",
    "    for line in file:\n",
    "        # Identify and create Category element\n",
    "        if line.strip() and \"Category\" in line:\n",
    "            category = line.split()[0]\n",
    "            category_element = ET.SubElement(root, \"Category\", name=category)\n",
    "        elif line.startswith(\"<fs af=\"):\n",
    "            # Start a new FeatureSet element within the current category\n",
    "            fs_element = ET.SubElement(category_element, \"FeatureSet\")\n",
    "            fs_element.set(\"attributes\", line.strip())\n",
    "        elif \"Root will be substituted\" in line:\n",
    "            # Add a Root element if within a FeatureSet\n",
    "            if fs_element is not None:\n",
    "                root_element = ET.SubElement(fs_element, \"Root\")\n",
    "                root_element.text = line.strip().split()[-1]\n",
    "        elif \"Step\" in line or \"Direct Word Generation\" in line:\n",
    "            # Add a Rule element if within a FeatureSet\n",
    "            if fs_element is not None:\n",
    "                rule_element = ET.SubElement(fs_element, \"Rule\")\n",
    "                rule_element.text = line.strip()\n",
    "        elif line.strip():\n",
    "            # Add an Output element if within a FeatureSet\n",
    "            if fs_element is not None:\n",
    "                output_element = ET.SubElement(fs_element, \"Output\")\n",
    "                output_element.text = line.strip()\n",
    "\n",
    "# Write the XML tree to the output file\n",
    "tree = ET.ElementTree(root)\n",
    "tree.write(output_file, encoding='utf-8', xml_declaration=True)\n",
    "\n",
    "print(f\"XML file '{output_file}' generated successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "991807b4-cdab-464c-8b0c-1e33c4aae8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apertium-style XML file 'paradigm-bhojpuri-apertium.xml' generated successfully.\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Define file paths\n",
    "input_file = 'B_M_M_Word-generation-ver-1.9.0.txt'\n",
    "output_file = 'paradigm-bhojpuri-apertium.xml'\n",
    "\n",
    "# Create the root element for Apertium XML\n",
    "root = ET.Element(\"dictionary\")\n",
    "\n",
    "# Open and parse the file line by line\n",
    "with open(input_file, 'r', encoding='utf-8') as file:\n",
    "    pardef_element = None  # To track pardef elements\n",
    "    par_element = None     # To track par elements\n",
    "    \n",
    "    for line in file:\n",
    "        # Check for category to start a new pardef\n",
    "        if line.strip() and \"Category\" in line:\n",
    "            category = line.split()[0]\n",
    "            pardef_element = ET.SubElement(root, \"pardef\", name=category)\n",
    "        \n",
    "        elif line.startswith(\"<fs af=\"):\n",
    "            # Start a new par element within pardef with morphological attributes\n",
    "            if pardef_element is not None:\n",
    "                morph_attributes = line.strip()\n",
    "                par_element = ET.SubElement(pardef_element, \"par\")\n",
    "                \n",
    "                # Extract af attribute content and set as lemma/morph\n",
    "                af_content = morph_attributes.split(\"'\")[1]\n",
    "                lemma, pos, gender, num, pers, case, emph, mood = af_content.split(',')\n",
    "                \n",
    "                # Add lemma and tags in Apertium format\n",
    "                lemma_element = ET.SubElement(par_element, \"l\")\n",
    "                lemma_element.text = lemma\n",
    "                \n",
    "                tags_element = ET.SubElement(par_element, \"r\")\n",
    "                tags_element.text = f\"<pos>{pos}</pos><gen>{gender}</gen><num>{num}</num><pers>{pers}</pers><case>{case}</case>\"\n",
    "\n",
    "        elif \"Root will be substituted\" in line:\n",
    "            # Add root form based on substitution\n",
    "            if par_element is not None:\n",
    "                root_word = line.strip().split()[-1]\n",
    "                form_element = ET.SubElement(par_element, \"form\")\n",
    "                form_element.text = root_word\n",
    "        \n",
    "        elif \"Step\" in line or \"Direct Word Generation\" in line:\n",
    "            # Add rule as a comment in the par element for reference\n",
    "            if par_element is not None:\n",
    "                rule_comment = ET.Comment(line.strip())\n",
    "                par_element.append(rule_comment)\n",
    "        \n",
    "        elif line.strip():\n",
    "            # Handle additional forms or outputs if they appear as standalone text\n",
    "            if par_element is not None:\n",
    "                form_element = ET.SubElement(par_element, \"form\")\n",
    "                form_element.text = line.strip()\n",
    "\n",
    "# Write the XML to Apertium format output file\n",
    "tree = ET.ElementTree(root)\n",
    "tree.write(output_file, encoding='utf-8', xml_declaration=True)\n",
    "\n",
    "print(f\"Apertium-style XML file '{output_file}' generated successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c70c914-090d-452d-a24a-48d6069c9343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bhojpuri Apertium-style XML file 'Bhoj_paradigm-bhojpuri-apertium.xml' created successfully.\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# Define input and output file paths\n",
    "input_file = 'B_M_M_Word-generation-ver-1.9.0.txt'\n",
    "output_file = 'Bhoj_paradigm-bhojpuri-apertium.xml'\n",
    "\n",
    "# Placeholder dictionary mapping Hindi forms to Bhojpuri forms (to be populated as needed)\n",
    "hindi_to_bhojpuri = {\n",
    "    # \" \": \" \",\n",
    "    # Add mappings here\n",
    "}\n",
    "\n",
    "# Function to convert Hindi form to Bhojpuri form\n",
    "def convert_to_bhojpuri(hindi_word):\n",
    "    return hindi_to_bhojpuri.get(hindi_word, hindi_word)  # Use Bhojpuri equivalent if available\n",
    "\n",
    "# Function to find the longest common subsequence (LCS) for the root form\n",
    "def longest_common_subsequence(words):\n",
    "    if not words:\n",
    "        return \"\"\n",
    "    lcs = words[0]\n",
    "    for word in words[1:]:\n",
    "        matcher = SequenceMatcher(None, lcs, word)\n",
    "        match = matcher.find_longest_match(0, len(lcs), 0, len(word))\n",
    "        lcs = lcs[match.a: match.a + match.size]\n",
    "    return lcs\n",
    "\n",
    "# Create the root element for the Apertium XML structure\n",
    "root = ET.Element(\"dictionary\")\n",
    "\n",
    "# Add Bhojpuri-specific alphabet and sdefs based on the pattern in 'paradigm-hin.xml'\n",
    "alphabet = ET.SubElement(root, \"alphabet\")\n",
    "alphabet.text = \"\"\n",
    "\n",
    "sdefs = ET.SubElement(root, \"sdefs\")\n",
    "sdefs_content = {\n",
    "    \"n\": \"Noun\", \"sg\": \"Singular\", \"pl\": \"Plural\",\n",
    "    \"m\": \"Masculine\", \"f\": \"Feminine\", \"o\": \"Oblique\", \"d\": \"Direct\",\n",
    "    \"p1\": \"First Person\", \"p2\": \"Second Person\", \"p3\": \"Third Person\"\n",
    "}\n",
    "for key, value in sdefs_content.items():\n",
    "    ET.SubElement(sdefs, \"sdef\", n=key, c=value)\n",
    "\n",
    "# Open the input file to parse data line by line\n",
    "with open(input_file, 'r', encoding='utf-8') as file:\n",
    "    pardef = None\n",
    "    par_element = None\n",
    "    word_forms = []  # Collect word forms for LCS/root calculation\n",
    "\n",
    "    for line in file:\n",
    "        # Check if line indicates a new category for pardef\n",
    "        if line.strip() and \"Category\" in line:\n",
    "            category_name = line.split()[0]\n",
    "            pardef = ET.SubElement(root, \"pardef\", name=category_name)\n",
    "            par_element = None  # Reset par_element for each new category\n",
    "            word_forms.clear()  # Reset word forms for new category\n",
    "        \n",
    "        # Identify morphological information lines and structure them in Apertium XML format\n",
    "        elif line.startswith(\"<fs af=\"):\n",
    "            if pardef is not None:\n",
    "                par_element = ET.SubElement(pardef, \"par\")\n",
    "                af_content = line.strip().split(\"'\")[1]\n",
    "                lemma, pos, gender, num, pers, case, _, mood = af_content.split(',')\n",
    "\n",
    "                l_element = ET.SubElement(par_element, \"l\")\n",
    "                # Convert Hindi lemma to Bhojpuri if available\n",
    "                l_element.text = convert_to_bhojpuri(lemma)\n",
    "\n",
    "                r_element = ET.SubElement(par_element, \"r\")\n",
    "                # Append features as Apertium-specific tags\n",
    "                for tag in [pos, gender, num, pers, case, mood]:\n",
    "                    if tag:\n",
    "                        ET.SubElement(r_element, \"s\", n=tag)\n",
    "        \n",
    "        # Handle root word substitution or additional forms\n",
    "        elif \"Root will be substituted\" in line:\n",
    "            if par_element is not None:\n",
    "                root_form = line.strip().split()[-1]\n",
    "                # Convert root form from Hindi to Bhojpuri\n",
    "                form = ET.SubElement(par_element, \"form\")\n",
    "                bhojpuri_root = convert_to_bhojpuri(root_form)\n",
    "                form.text = bhojpuri_root\n",
    "                word_forms.append(bhojpuri_root)\n",
    "\n",
    "        elif \"Step\" in line or \"Direct Word Generation\" in line:\n",
    "            # Sanitize double hyphens in comments\n",
    "            if par_element is not None:\n",
    "                sanitized_comment = line.replace('--', '')  # Replace double hyphen with em dash\n",
    "                sanitized_comment = sanitized_comment.replace('<', '&lt;').replace('>', '&gt;')  # Sanitize XML characters\n",
    "                step_comment = ET.Comment(sanitized_comment)\n",
    "                par_element.append(step_comment)\n",
    "        \n",
    "        elif line.strip():\n",
    "            # Additional forms are added as 'form' elements if par_element exists\n",
    "            if par_element is not None:\n",
    "                bhojpuri_form = convert_to_bhojpuri(line.strip())\n",
    "                form = ET.SubElement(par_element, \"form\")\n",
    "                form.text = bhojpuri_form\n",
    "                word_forms.append(bhojpuri_form)\n",
    "\n",
    "    # Calculate the root form as the longest common subsequence (LCS)\n",
    "    if word_forms:\n",
    "        lcs_root = longest_common_subsequence(word_forms)\n",
    "        if lcs_root:\n",
    "            root_element = ET.SubElement(root, \"root\")\n",
    "            root_element.text = lcs_root\n",
    "\n",
    "# Write the Apertium Bhojpuri XML output file\n",
    "tree = ET.ElementTree(root)\n",
    "tree.write(output_file, encoding='utf-8', xml_declaration=True)\n",
    "\n",
    "print(f\"Bhojpuri Apertium-style XML file '{output_file}' created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4993f57b-5223-4133-97a4-9bf3eb685983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bhojpuri Apertium-style XML file '000Bhojpuri_paradigm_apertium.xml' created successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Define file paths\n",
    "input_excel = 'word_data.xlsx'  # The Excel file with categories, roots, and word forms\n",
    "output_file = '000Bhojpuri_paradigm_apertium.xml'\n",
    "\n",
    "# Load the Excel file into a DataFrame\n",
    "df = pd.read_excel(input_excel)\n",
    "\n",
    "# Create the root element for the XML structure\n",
    "root = ET.Element(\"dictionary\")\n",
    "\n",
    "# Add Bhojpuri-specific alphabet and sdefs (grammatical tags)\n",
    "alphabet = ET.SubElement(root, \"alphabet\")\n",
    "alphabet.text = \"\"\n",
    "\n",
    "sdefs = ET.SubElement(root, \"sdefs\")\n",
    "sdefs_content = {\n",
    "    \"n\": \"Noun\", \"sg\": \"Singular\", \"pl\": \"Plural\",\n",
    "    \"m\": \"Masculine\", \"f\": \"Feminine\", \"o\": \"Oblique\", \"d\": \"Direct\",\n",
    "    \"p1\": \"First Person\", \"p2\": \"Second Person\", \"p3\": \"Third Person\"\n",
    "}\n",
    "for key, value in sdefs_content.items():\n",
    "    ET.SubElement(sdefs, \"sdef\", n=key, c=value)\n",
    "\n",
    "# Iterate through each row in the DataFrame to create paradigms\n",
    "for _, row in df.iterrows():\n",
    "    category = row[\"Category\"]\n",
    "    root_word = row[\"Root\"]\n",
    "\n",
    "    # Create a pardef element for each root word within its category\n",
    "    pardef = ET.SubElement(root, \"pardef\", name=f\"{category}_{root_word}\")\n",
    "\n",
    "    # Create par entries for each word form and add it to pardef\n",
    "    for form_num in range(1, 5):\n",
    "        word_form_column = f\"Word Form {form_num}\"\n",
    "        \n",
    "        if word_form_column in row and pd.notna(row[word_form_column]):\n",
    "            word_form = row[word_form_column]\n",
    "            \n",
    "            # Each word form has a par element with l (lemma) and r (root reference)\n",
    "            par_element = ET.SubElement(pardef, \"par\")\n",
    "            \n",
    "            # l element represents the root form (lemma)\n",
    "            l_element = ET.SubElement(par_element, \"l\")\n",
    "            l_element.text = root_word\n",
    "            \n",
    "            # r element represents each specific word form\n",
    "            r_element = ET.SubElement(par_element, \"r\")\n",
    "            r_element.text = word_form\n",
    "\n",
    "# Write the XML structure to file in Apertium format\n",
    "tree = ET.ElementTree(root)\n",
    "tree.write(output_file, encoding='utf-8', xml_declaration=True)\n",
    "\n",
    "print(f\"Bhojpuri Apertium-style XML file '{output_file}' created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9128cf9-a569-406a-a325-1d573ae2f9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XML file successfully created at 01001bhojpuri_dictionary.xml\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "excel_file_path = 'word_data.xlsx'\n",
    "word_data = pd.read_excel(excel_file_path)\n",
    "\n",
    "# Function to create a lexical entry with associated word forms\n",
    "def create_lexical_entry(root, word_root, word_forms, category):\n",
    "    entry = ET.SubElement(root, \"lexical_entry\")\n",
    "    lemma = ET.SubElement(entry, \"lemma\", form=word_root)\n",
    "    attributes = ET.SubElement(lemma, \"attributes\")\n",
    "    ET.SubElement(attributes, \"category\").text = category\n",
    "    \n",
    "    forms = ET.SubElement(entry, \"word_forms\")\n",
    "    for form in word_forms:\n",
    "        ET.SubElement(forms, \"form\", value=form)\n",
    "\n",
    "# Create XML structure\n",
    "root = ET.Element(\"dictionary\")\n",
    "for _, row in word_data.iterrows():\n",
    "    word_root = row['Root']\n",
    "    word_forms = [row['Word Form 1'], row['Word Form 2'], row['Word Form 3'] ,row['Word Form 4']]\n",
    "    category = row['Category']\n",
    "    create_lexical_entry(root, word_root, word_forms, category)\n",
    "\n",
    "# Save the XML to a file\n",
    "output_path = \"01001bhojpuri_dictionary.xml\"\n",
    "tree = ET.ElementTree(root)\n",
    "tree.write(output_path, encoding=\"utf-8\", xml_declaration=True)\n",
    "\n",
    "print(f\"XML file successfully created at {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "caecda65-80ab-4f0d-a8cb-bbac831d573c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bhojpuri paradigm XML file successfully created at 0056paradigm-bhojpuri78.xml\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "# Load data from the Excel file\n",
    "excel_file_path = 'word_data.xlsx'\n",
    "word_data = pd.read_excel(excel_file_path)\n",
    "\n",
    "# Define Bhojpuri-specific alphabet\n",
    "alphabet = \"\"\n",
    "\n",
    "# Create the XML structure\n",
    "root = ET.Element(\"dictionary\")\n",
    "\n",
    "# Add alphabet section\n",
    "alphabet_elem = ET.SubElement(root, \"alphabet\")\n",
    "alphabet_elem.text = alphabet\n",
    "\n",
    "# Add sdefs (grammatical definitions)\n",
    "sdefs = ET.SubElement(root, \"sdefs\")\n",
    "grammatical_features = [\n",
    "    (\"n\", \"Noun\"),\n",
    "    (\"sg\", \"Singular\"),\n",
    "    (\"pl\", \"Plural\"),\n",
    "    (\"m\", \"Masculine\"),\n",
    "    (\"f\", \"Feminine\"),\n",
    "    (\"o\", \"Oblique\"),\n",
    "    (\"d\", \"Direct\"),\n",
    "    (\"p3\", \"Third Person\"),\n",
    "]\n",
    "for name, description in grammatical_features:\n",
    "    ET.SubElement(sdefs, \"sdef\", n=name, c=description)\n",
    "\n",
    "# Add pardefs section\n",
    "pardefs = ET.SubElement(root, \"pardefs\")\n",
    "\n",
    "# Populate pardefs based on the Excel file\n",
    "for _, row in word_data.iterrows():\n",
    "    category = row[\"Category\"]  # Grammatical category (e.g., Noun_m)\n",
    "    root_word = row[\"Root\"]  # Root form\n",
    "    word_forms = [\n",
    "        row[\"Word Form 1\"], \n",
    "        row[\"Word Form 2\"], \n",
    "        row[\"Word Form 3\"], \n",
    "        row[\"Word Form 4\"],\n",
    "    ]\n",
    "    \n",
    "    # Create pardef for each root\n",
    "    pardef = ET.SubElement(pardefs, \"pardef\", n=f\"{root_word}__{category}\")\n",
    "    \n",
    "    for form in word_forms:\n",
    "        e = ET.SubElement(pardef, \"e\")\n",
    "        p = ET.SubElement(e, \"p\")\n",
    "        l = ET.SubElement(p, \"l\")\n",
    "        r = ET.SubElement(p, \"r\")\n",
    "        \n",
    "        # Add suffix (`<l>`) and root word (`<r>`)\n",
    "        l.text = form[len(root_word):] if form.startswith(root_word) else form\n",
    "        r.text = root_word\n",
    "\n",
    "        # Add grammatical attributes (`<s>`)\n",
    "        ET.SubElement(r, \"s\", n=\"n\")  # Noun\n",
    "        if \"m\" in category.lower():\n",
    "            ET.SubElement(r, \"s\", n=\"m\")  # Masculine\n",
    "        elif \"f\" in category.lower():\n",
    "            ET.SubElement(r, \"s\", n=\"f\")  # Feminine\n",
    "        if \"sg\" in form.lower():\n",
    "            ET.SubElement(r, \"s\", n=\"sg\")\n",
    "        else:\n",
    "            ET.SubElement(r, \"s\", n=\"pl\")\n",
    "        ET.SubElement(r, \"s\", n=\"p3\")  # Third person\n",
    "        ET.SubElement(r, \"s\", n=\"d\")  # Direct case\n",
    "\n",
    "# Save the XML file\n",
    "output_path = \"0056paradigm-bhojpuri78.xml\"\n",
    "tree = ET.ElementTree(root)\n",
    "tree.write(output_path, encoding=\"utf-8\", xml_declaration=True)\n",
    "\n",
    "print(f\"Bhojpuri paradigm XML file successfully created at {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39b8eac7-e70b-47f7-b95a-c8d27d7499c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bhojpuri paradigm XML file created at bhojpuri_paradigm.xml\n",
      "Common sequences XML file created at common_sequences.xml\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "# Load data from the Excel file\n",
    "excel_file_path = 'word_data.xlsx'\n",
    "word_data = pd.read_excel(excel_file_path)\n",
    "\n",
    "# Function to find common prefix or suffix sequences\n",
    "def find_common_sequence(word_forms):\n",
    "    \"\"\"Find common prefix or suffix among a list of word forms.\"\"\"\n",
    "    if not word_forms:\n",
    "        return \"\"\n",
    "    \n",
    "    # Find the longest common prefix\n",
    "    prefix = word_forms[0]\n",
    "    for word in word_forms[1:]:\n",
    "        # Compare prefix length\n",
    "        i = 0\n",
    "        while i < len(prefix) and i < len(word) and prefix[i] == word[i]:\n",
    "            i += 1\n",
    "        prefix = prefix[:i]\n",
    "    \n",
    "    # Check if we have a common prefix or suffix\n",
    "    return prefix\n",
    "\n",
    "# Create XML structure for Bhojpuri paradigm file\n",
    "root = ET.Element(\"dictionary\")\n",
    "\n",
    "# Define Bhojpuri-specific alphabet\n",
    "alphabet = \"\"\n",
    "\n",
    "# Add alphabet section\n",
    "alphabet_elem = ET.SubElement(root, \"alphabet\")\n",
    "alphabet_elem.text = alphabet\n",
    "\n",
    "# Add sdefs (grammatical definitions)\n",
    "sdefs = ET.SubElement(root, \"sdefs\")\n",
    "grammatical_features = [\n",
    "    (\"n\", \"Noun\"),\n",
    "    (\"sg\", \"Singular\"),\n",
    "    (\"pl\", \"Plural\"),\n",
    "    (\"m\", \"Masculine\"),\n",
    "    (\"f\", \"Feminine\"),\n",
    "    (\"o\", \"Oblique\"),\n",
    "    (\"d\", \"Direct\"),\n",
    "    (\"p3\", \"Third Person\"),\n",
    "]\n",
    "for name, description in grammatical_features:\n",
    "    ET.SubElement(sdefs, \"sdef\", n=name, c=description)\n",
    "\n",
    "# Create XML structure for common sequences\n",
    "common_sequences_root = ET.Element(\"common_sequences\")\n",
    "common_sequences = {}\n",
    "\n",
    "# Add pardefs section\n",
    "pardefs = ET.SubElement(root, \"pardefs\")\n",
    "\n",
    "# Populate pardefs based on the Excel file\n",
    "for _, row in word_data.iterrows():\n",
    "    category = row[\"Category\"]  # Grammatical category (e.g., Noun_m)\n",
    "    root_word = row[\"Root\"]  # Root form\n",
    "    word_forms = [\n",
    "        row[\"Word Form 1\"], \n",
    "        row[\"Word Form 2\"], \n",
    "        row[\"Word Form 3\"], \n",
    "        row[\"Word Form 4\"],\n",
    "    ]\n",
    "    \n",
    "    # Find common subsequence in the word forms\n",
    "    common_sequence = find_common_sequence(word_forms)\n",
    "    \n",
    "    if common_sequence:\n",
    "        common_sequences[common_sequence] = {\n",
    "            \"root\": root_word,\n",
    "            \"category\": category,\n",
    "            \"word_forms\": word_forms\n",
    "        }\n",
    "    \n",
    "    # Create pardef for each root\n",
    "    pardef = ET.SubElement(pardefs, \"pardef\", n=f\"{root_word}__{category}\")\n",
    "    \n",
    "    for form in word_forms:\n",
    "        e = ET.SubElement(pardef, \"e\")\n",
    "        p = ET.SubElement(e, \"p\")\n",
    "        l = ET.SubElement(p, \"l\")\n",
    "        r = ET.SubElement(p, \"r\")\n",
    "        \n",
    "        # Add suffix (`<l>`) and root word (`<r>`)\n",
    "        l.text = form[len(root_word):] if form.startswith(root_word) else form\n",
    "        r.text = root_word\n",
    "\n",
    "        # Add grammatical attributes (`<s>`)\n",
    "        ET.SubElement(r, \"s\", n=\"n\")  # Noun\n",
    "        if \"m\" in category.lower():\n",
    "            ET.SubElement(r, \"s\", n=\"m\")  # Masculine\n",
    "        elif \"f\" in category.lower():\n",
    "            ET.SubElement(r, \"s\", n=\"f\")  # Feminine\n",
    "        if \"sg\" in form.lower():\n",
    "            ET.SubElement(r, \"s\", n=\"sg\")\n",
    "        else:\n",
    "            ET.SubElement(r, \"s\", n=\"pl\")\n",
    "        ET.SubElement(r, \"s\", n=\"p3\")  # Third person\n",
    "        ET.SubElement(r, \"s\", n=\"d\")  # Direct case\n",
    "\n",
    "# Save the common sequences to another XML file\n",
    "common_sequences_elem = ET.ElementTree(common_sequences_root)\n",
    "for sequence, words_info in common_sequences.items():\n",
    "    seq_elem = ET.SubElement(common_sequences_root, \"sequence\", value=sequence)\n",
    "    word_elem = ET.SubElement(seq_elem, \"word\")\n",
    "    root_elem = ET.SubElement(word_elem, \"root\")\n",
    "    root_elem.text = words_info[\"root\"]\n",
    "    category_elem = ET.SubElement(word_elem, \"category\")\n",
    "    category_elem.text = words_info[\"category\"]\n",
    "    forms_elem = ET.SubElement(word_elem, \"word_forms\")\n",
    "    forms_elem.text = \", \".join(words_info[\"word_forms\"])\n",
    "\n",
    "# Save the Bhojpuri appretium XML\n",
    "output_path = \"bhojpuri_paradigm.xml\"\n",
    "tree = ET.ElementTree(root)\n",
    "tree.write(output_path, encoding=\"utf-8\", xml_declaration=True)\n",
    "\n",
    "# Save the common sequences XML\n",
    "common_sequences_path = \"common_sequences.xml\"\n",
    "common_sequences_elem.write(common_sequences_path, encoding=\"utf-8\", xml_declaration=True)\n",
    "\n",
    "print(f\"Bhojpuri paradigm XML file created at {output_path}\")\n",
    "print(f\"Common sequences XML file created at {common_sequences_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c18d54b-ca3c-42ca-be29-4192bd844cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<?xml version=\\'1.0\\' encoding=\\'utf-8\\'?> <dictionary><section type=\"Noun_m\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_m\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_m\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_m\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_m\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_m\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_m\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_m\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_m\"><e><p n=\"l\"></p><p n=\"form1\">///</p><p n=\"form2\">///</p><p n=\"form3\">///</p><p n=\"form4\">///</p></e></section><section type=\"Noun_m\"><e><p n=\"l\"></p><p n=\"form1\">/</p><p n=\"form2\">/</p><p n=\"form3\">/</p><p n=\"form4\">/</p></e></section><section type=\"Noun_m\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_m\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\">/</p><p n=\"form3\"></p><p n=\"form4\">/</p></e></section><section type=\"Noun_m\"><e><p n=\"l\"></p><p n=\"form1\">/</p><p n=\"form2\">///</p><p n=\"form3\">/</p><p n=\"form4\">///</p></e></section><section type=\"Noun_m\"><e><p n=\"l\"></p><p n=\"form1\">/</p><p n=\"form2\">/</p><p n=\"form3\">/</p><p n=\"form4\">/</p></e></section><section type=\"Noun_m\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_m_e\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">//</p><p n=\"form3\">//</p><p n=\"form4\">//</p></e></section><section type=\"Noun_m_e\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">//</p><p n=\"form3\">//</p><p n=\"form4\">//</p></e></section><section type=\"Noun_m_e\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">//</p><p n=\"form3\">//</p><p n=\"form4\">//</p></e></section><section type=\"Noun_m_e\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">//</p><p n=\"form3\">//</p><p n=\"form4\">//</p></e></section><section type=\"Noun_m_e\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">//</p><p n=\"form3\">//</p><p n=\"form4\">//</p></e></section><section type=\"Noun_m_e\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">//</p><p n=\"form3\">//</p><p n=\"form4\">//</p></e></section><section type=\"Noun_m_e\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">//</p><p n=\"form3\">//</p><p n=\"form4\">//</p></e></section><section type=\"Noun_m_e\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">//</p><p n=\"form3\">//</p><p n=\"form4\">//</p></e></section><section type=\"Noun_m_e\"><e><p n=\"l\"></p><p n=\"form1\">///////////</p><p n=\"form2\">///////////</p><p n=\"form3\">///////////</p><p n=\"form4\">///////////</p></e></section><section type=\"Noun_m_e\"><e><p n=\"l\"></p><p n=\"form1\">/////</p><p n=\"form2\">/////</p><p n=\"form3\">/////</p><p n=\"form4\">/////</p></e></section><section type=\"Noun_m_e\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">//</p><p n=\"form3\">//</p><p n=\"form4\">//</p></e></section><section type=\"Noun_m_e\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">/////</p><p n=\"form3\">//</p><p n=\"form4\">/////</p></e></section><section type=\"Noun_m_e\"><e><p n=\"l\"></p><p n=\"form1\">/////</p><p n=\"form2\">/////</p><p n=\"form3\">/////</p><p n=\"form4\">/////</p></e></section><section type=\"Noun_m_e\"><e><p n=\"l\"></p><p n=\"form1\">/////</p><p n=\"form2\">/////</p><p n=\"form3\">/////</p><p n=\"form4\">/////</p></e></section><section type=\"Noun_m_e\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">//</p><p n=\"form3\">//</p><p n=\"form4\">//</p></e></section><section type=\"Noun_m_e1\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">//</p><p n=\"form3\">//</p><p n=\"form4\">//</p></e></section><section type=\"Noun_m_e1\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">//</p><p n=\"form3\">//</p><p n=\"form4\">//</p></e></section><section type=\"Noun_m_e1\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">//</p><p n=\"form3\">//</p><p n=\"form4\">//</p></e></section><section type=\"Noun_m_e1\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">//</p><p n=\"form3\">//</p><p n=\"form4\">//</p></e></section><section type=\"Noun_m_e1\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">//</p><p n=\"form3\">//</p><p n=\"form4\">//</p></e></section><section type=\"Noun_m_e1\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">//</p><p n=\"form3\">//</p><p n=\"form4\">//</p></e></section><section type=\"Noun_m_e1\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">//</p><p n=\"form3\">//</p><p n=\"form4\">//</p></e></section><section type=\"Noun_m_e1\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">//</p><p n=\"form3\">//</p><p n=\"form4\">//</p></e></section><section type=\"Noun_m_e1\"><e><p n=\"l\"></p><p n=\"form1\">///////////</p><p n=\"form2\">///////////</p><p n=\"form3\">///////////</p><p n=\"form4\">///////////</p></e></section><section type=\"Noun_m_e1\"><e><p n=\"l\"></p><p n=\"form1\">/////</p><p n=\"form2\">/////</p><p n=\"form3\">/////</p><p n=\"form4\">/////</p></e></section><section type=\"Noun_m_e1\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">//</p><p n=\"form3\">//</p><p n=\"form4\">//</p></e></section><section type=\"Noun_m_e1\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">/////</p><p n=\"form3\">//</p><p n=\"form4\">/////</p></e></section><section type=\"Noun_m_e1\"><e><p n=\"l\"></p><p n=\"form1\">/////</p><p n=\"form2\">/////</p><p n=\"form3\">/////</p><p n=\"form4\">/////</p></e></section><section type=\"Noun_m_e1\"><e><p n=\"l\"></p><p n=\"form1\">/////</p><p n=\"form2\">/////</p><p n=\"form3\">/////</p><p n=\"form4\">/////</p></e></section><section type=\"Noun_m_e1\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">//</p><p n=\"form3\">//</p><p n=\"form4\">//</p></e></section><section type=\"Noun_f\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_f\"><e><p n=\"l\"></p><p n=\"form1\">/</p><p n=\"form2\">/</p><p n=\"form3\">/</p><p n=\"form4\">/</p></e></section><section type=\"Noun_f\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_f\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_f\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_f\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_f\"><e><p n=\"l\"></p><p n=\"form1\">/</p><p n=\"form2\">/</p><p n=\"form3\">/</p><p n=\"form4\">/</p></e></section><section type=\"Noun_f\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_f\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_f\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_f\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_f\"><e><p n=\"l\"></p><p n=\"form1\">/</p><p n=\"form2\">/////</p><p n=\"form3\">/</p><p n=\"form4\">/////</p></e></section><section type=\"Noun_f\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_f\"><e><p n=\"l\"></p><p n=\"form1\">/</p><p n=\"form2\">/</p><p n=\"form3\">/</p><p n=\"form4\">/</p></e></section><section type=\"Noun_f\"><e><p n=\"l\"></p><p n=\"form1\">///</p><p n=\"form2\">///</p><p n=\"form3\">///</p><p n=\"form4\">///</p></e></section><section type=\"Noun_f\"><e><p n=\"l\"></p><p n=\"form1\">/</p><p n=\"form2\">///</p><p n=\"form3\">/</p><p n=\"form4\">///</p></e></section><section type=\"Noun_f\"><e><p n=\"l\"></p><p n=\"form1\">/</p><p n=\"form2\">/</p><p n=\"form3\">/</p><p n=\"form4\">/</p></e></section><section type=\"Noun_f\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_f_e\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_f_e\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_f_e\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_f_e\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_f_e\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_f_e\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_f_e\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_f_e\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_f_e\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_f_e\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_f_e\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_f_e\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_f_e\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_f_e\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_f_e\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_f_e\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_f_e\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_f_e\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_f_e\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_f_e\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_f_e\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_f_e\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_f_e\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_f_e\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_f_e\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_f_e\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_f_e\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_f_e\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_m_e1\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">//</p><p n=\"form3\">//</p><p n=\"form4\">//</p></e></section><section type=\"Noun_m_e1\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">//</p><p n=\"form3\">//</p><p n=\"form4\">//</p></e></section><section type=\"Noun_m_e1\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">//</p><p n=\"form3\">//</p><p n=\"form4\">//</p></e></section><section type=\"Noun_m_e1\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">//</p><p n=\"form3\">//</p><p n=\"form4\">//</p></e></section><section type=\"Noun_m_e1\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">//</p><p n=\"form3\">//</p><p n=\"form4\">//</p></e></section><section type=\"Noun_m_e1\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">//</p><p n=\"form3\">//</p><p n=\"form4\">//</p></e></section><section type=\"Noun_m_e1\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">//</p><p n=\"form3\">//</p><p n=\"form4\">//</p></e></section><section type=\"Noun_m_e1\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">//</p><p n=\"form3\">//</p><p n=\"form4\">//</p></e></section><section type=\"Noun_m_e1\"><e><p n=\"l\"></p><p n=\"form1\">///////////</p><p n=\"form2\">///////////</p><p n=\"form3\">///////////</p><p n=\"form4\">///////////</p></e></section><section type=\"Noun_m_e1\"><e><p n=\"l\"></p><p n=\"form1\">/////</p><p n=\"form2\">/////</p><p n=\"form3\">/////</p><p n=\"form4\">/////</p></e></section><section type=\"Noun_m_e1\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">//</p><p n=\"form3\">//</p><p n=\"form4\">//</p></e></section><section type=\"Noun_m_e1\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">/////</p><p n=\"form3\">//</p><p n=\"form4\">/////</p></e></section><section type=\"Noun_m_e1\"><e><p n=\"l\"></p><p n=\"form1\">/////</p><p n=\"form2\">/////</p><p n=\"form3\">/////</p><p n=\"form4\">/////</p></e></section><section type=\"Noun_m_e1\"><e><p n=\"l\"></p><p n=\"form1\">/////</p><p n=\"form2\">/////</p><p n=\"form3\">/////</p><p n=\"form4\">/////</p></e></section><section type=\"Noun_m_e1\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">//</p><p n=\"form3\">//</p><p n=\"form4\">//</p></e></section><section type=\"Noun_f_rednt\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\">/</p><p n=\"form3\"></p><p n=\"form4\">/</p></e></section><section type=\"Noun_f_rednt\"><e><p n=\"l\"></p><p n=\"form1\">/</p><p n=\"form2\">/</p><p n=\"form3\">/</p><p n=\"form4\">/</p></e></section><section type=\"Noun_f_rednt\"><e><p n=\"l\"></p><p n=\"form1\">/</p><p n=\"form2\">/</p><p n=\"form3\">/</p><p n=\"form4\">/</p></e></section><section type=\"Noun_f_rednt\"><e><p n=\"l\"></p><p n=\"form1\">/</p><p n=\"form2\">/</p><p n=\"form3\">/</p><p n=\"form4\">/</p></e></section><section type=\"Noun_f_rednt\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_f_rednt\"><e><p n=\"l\"></p><p n=\"form1\">/</p><p n=\"form2\">/</p><p n=\"form3\">/</p><p n=\"form4\">/</p></e></section><section type=\"Noun_f_rednt\"><e><p n=\"l\"></p><p n=\"form1\">/</p><p n=\"form2\">/</p><p n=\"form3\">/</p><p n=\"form4\">/</p></e></section><section type=\"Noun_f_rednt\"><e><p n=\"l\"></p><p n=\"form1\">/</p><p n=\"form2\">/</p><p n=\"form3\">/</p><p n=\"form4\">/</p></e></section><section type=\"Noun_f_rednt\"><e><p n=\"l\"></p><p n=\"form1\">/</p><p n=\"form2\">/</p><p n=\"form3\">/</p><p n=\"form4\">/</p></e></section><section type=\"Noun_f_rednt\"><e><p n=\"l\"></p><p n=\"form1\">/</p><p n=\"form2\">/</p><p n=\"form3\">/</p><p n=\"form4\">/</p></e></section><section type=\"Noun_f_rednt\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\"></p></e></section><section type=\"Noun_f_rednt\"><e><p n=\"l\"></p><p n=\"form1\">/</p><p n=\"form2\">/</p><p n=\"form3\">/</p><p n=\"form4\">///</p></e></section><section type=\"Noun_f_rednt\"><e><p n=\"l\"></p><p n=\"form1\">/</p><p n=\"form2\">/</p><p n=\"form3\">/</p><p n=\"form4\">/</p></e></section><section type=\"Noun_f_rednt\"><e><p n=\"l\"></p><p n=\"form1\">/</p><p n=\"form2\">/</p><p n=\"form3\">/</p><p n=\"form4\">///</p></e></section><section type=\"Noun_f_rednt\"><e><p n=\"l\"></p><p n=\"form1\">///</p><p n=\"form2\">///</p><p n=\"form3\">///</p><p n=\"form4\">///</p></e></section><section type=\"Noun_f_rednt\"><e><p n=\"l\"></p><p n=\"form1\"></p><p n=\"form2\"></p><p n=\"form3\"></p><p n=\"form4\">/</p></e></section><section type=\"Noun_f_rednt_e\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">/////</p><p n=\"form3\">//</p><p n=\"form4\">/////</p></e></section><section type=\"Noun_f_rednt_e\"><e><p n=\"l\"></p><p n=\"form1\">/////</p><p n=\"form2\">/////</p><p n=\"form3\">/////</p><p n=\"form4\">/////</p></e></section><section type=\"Noun_f_rednt_e\"><e><p n=\"l\"></p><p n=\"form1\">/////</p><p n=\"form2\">/////</p><p n=\"form3\">/////</p><p n=\"form4\">/////</p></e></section><section type=\"Noun_f_rednt_e\"><e><p n=\"l\"></p><p n=\"form1\">/////</p><p n=\"form2\">/////</p><p n=\"form3\">/////</p><p n=\"form4\">/////</p></e></section><section type=\"Noun_f_rednt_e\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">//</p><p n=\"form3\">//</p><p n=\"form4\">//</p></e></section><section type=\"Noun_f_rednt_e\"><e><p n=\"l\"></p><p n=\"form1\">/////</p><p n=\"form2\">/////</p><p n=\"form3\">/////</p><p n=\"form4\">/////</p></e></section><section type=\"Noun_f_rednt_e\"><e><p n=\"l\"></p><p n=\"form1\">/////</p><p n=\"form2\">/////</p><p n=\"form3\">/////</p><p n=\"form4\">/////</p></e></section><section type=\"Noun_f_rednt_e\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">//</p><p n=\"form3\">//</p><p n=\"form4\">//</p></e></section><section type=\"Noun_f_rednt_e\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">//</p><p n=\"form3\">//</p><p n=\"form4\">//</p></e></section><section type=\"Noun_f_rednt_e\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">//</p><p n=\"form3\">//</p><p n=\"form4\">//</p></e></section><section type=\"Noun_f_rednt_e\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">//</p><p n=\"form3\">//</p><p n=\"form4\">//</p></e></section><section type=\"Noun_f_rednt_e\"><e><p n=\"l\"></p><p n=\"form1\">///////////</p><p n=\"form2\">///////////</p><p n=\"form3\">///////////</p><p n=\"form4\">///////////</p></e></section><section type=\"Noun_f_rednt_e\"><e><p n=\"l\"></p><p n=\"form1\">///////////</p><p n=\"form2\">/////</p><p n=\"form3\">///////////</p><p n=\"form4\">//////</p></e></section><section type=\"Noun_f_rednt_e\"><e><p n=\"l\"></p><p n=\"form1\">///////////</p><p n=\"form2\">///////////</p><p n=\"form3\">///////////</p><p n=\"form4\">/////</p></e></section><section type=\"Noun_f_rednt_e\"><e><p n=\"l\"></p><p n=\"form1\">///////////</p><p n=\"form2\">/////</p><p n=\"form3\">///////////</p><p n=\"form4\">/////</p></e></section><section type=\"Noun_f_rednt_e\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">//</p><p n=\"form3\">//</p><p n=\"form4\">//</p></e></section><section type=\"Noun_f_rednt_e1\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">/////</p><p n=\"form3\">//</p><p n=\"form4\">/////</p></e></section><section type=\"Noun_f_rednt_e1\"><e><p n=\"l\"></p><p n=\"form1\">/////</p><p n=\"form2\">/////</p><p n=\"form3\">/////</p><p n=\"form4\">/////</p></e></section><section type=\"Noun_f_rednt_e1\"><e><p n=\"l\"></p><p n=\"form1\">/////</p><p n=\"form2\">/////</p><p n=\"form3\">/////</p><p n=\"form4\">/////</p></e></section><section type=\"Noun_f_rednt_e1\"><e><p n=\"l\"></p><p n=\"form1\">/////</p><p n=\"form2\">/////</p><p n=\"form3\">/////</p><p n=\"form4\">/////</p></e></section><section type=\"Noun_f_rednt_e1\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">//</p><p n=\"form3\">//</p><p n=\"form4\">//</p></e></section><section type=\"Noun_f_rednt_e1\"><e><p n=\"l\"></p><p n=\"form1\">/////</p><p n=\"form2\">//////</p><p n=\"form3\">/////</p><p n=\"form4\">//////</p></e></section><section type=\"Noun_f_rednt_e1\"><e><p n=\"l\"></p><p n=\"form1\">/////</p><p n=\"form2\">/////</p><p n=\"form3\">/////</p><p n=\"form4\">/////</p></e></section><section type=\"Noun_f_rednt_e1\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">//</p><p n=\"form3\">//</p><p n=\"form4\">//</p></e></section><section type=\"Noun_f_rednt_e1\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">//</p><p n=\"form3\">//</p><p n=\"form4\">//</p></e></section><section type=\"Noun_f_rednt_e1\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">//</p><p n=\"form3\">//</p><p n=\"form4\">//</p></e></section><section type=\"Noun_f_rednt_e1\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">//</p><p n=\"form3\">//</p><p n=\"form4\">//</p></e></section><section type=\"Noun_f_rednt_e1\"><e><p n=\"l\"></p><p n=\"form1\">///////////</p><p n=\"form2\">///////////</p><p n=\"form3\">///////////</p><p n=\"form4\">///////////</p></e></section><section type=\"Noun_f_rednt_e1\"><e><p n=\"l\"></p><p n=\"form1\">///////////</p><p n=\"form2\">/////</p><p n=\"form3\">///////////</p><p n=\"form4\">/////</p></e></section><section type=\"Noun_f_rednt_e1\"><e><p n=\"l\"></p><p n=\"form1\">///////////</p><p n=\"form2\">///////////</p><p n=\"form3\">///////////</p><p n=\"form4\">/////</p></e></section><section type=\"Noun_f_rednt_e1\"><e><p n=\"l\"></p><p n=\"form1\">///////////</p><p n=\"form2\">/////</p><p n=\"form3\">///////////</p><p n=\"form4\">/////</p></e></section><section type=\"Noun_f_rednt_e1\"><e><p n=\"l\"></p><p n=\"form1\">//</p><p n=\"form2\">//</p><p n=\"form3\">//</p><p n=\"form4\">//</p></e></section></dictionary>'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "def create_apertium_xml(input_file, output_file):\n",
    "    # Load the Excel file\n",
    "    data = pd.read_excel(input_file)\n",
    "\n",
    "    # Create the root element of the XML\n",
    "    root = ET.Element('dictionary')\n",
    "\n",
    "    # Iterate through the rows of the data\n",
    "    for _, row in data.iterrows():\n",
    "        category = row['Category']\n",
    "        root_word = row['Root']\n",
    "\n",
    "        # Create a section for each root word\n",
    "        entry = ET.SubElement(root, 'section', type=category)\n",
    "        lemma = ET.SubElement(entry, 'e')\n",
    "        ET.SubElement(lemma, 'p', n='l').text = root_word\n",
    "\n",
    "        # Add word forms\n",
    "        for i in range(1, 5):\n",
    "            word_form = row[f'Word Form {i}']\n",
    "            ET.SubElement(lemma, 'p', n=f'form{i}').text = word_form\n",
    "\n",
    "    # Write the XML tree to a file\n",
    "    tree = ET.ElementTree(root)\n",
    "    tree.write(output_file, encoding='utf-8', xml_declaration=True)\n",
    "\n",
    "def find_longest_subsequence(xml_file):\n",
    "    with open(xml_file, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    words = content.split()\n",
    "    longest_seq = ''\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        for j in range(i + 1, len(words) + 1):\n",
    "            subseq = ' '.join(words[i:j])\n",
    "            if len(subseq) > len(longest_seq):\n",
    "                longest_seq = subseq\n",
    "\n",
    "    return longest_seq\n",
    "\n",
    "# File paths\n",
    "input_excel = 'word_data.xlsx'\n",
    "output_xml = '#1234hojpuri_dictionary.xml'\n",
    "\n",
    "# Generate the XML file\n",
    "create_apertium_xml(input_excel, output_xml)\n",
    "\n",
    "# Find the longest subsequence in the XML file\n",
    "longest_sequence = find_longest_subsequence(output_xml)\n",
    "\n",
    "longest_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b376b85d-320e-4f28-8b69-966fdfaee963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to 'word_data_pronoun.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Input data\n",
    "pronoun_data = [\n",
    "    ('Apana hI',), ('apanahuz ke',), ('apano ke',), ('hamaniyoM ke',), ('hamanoM ke',), ('hamano ke',),\n",
    "    ('ihAM ke',), ('ihAz ke',), ('iheM ke',), ('ihez ke',), ('ihoM ke',), ('ihoz ke',),\n",
    "    ('uhAM ke',), ('uhAz ke',), ('uheM ke',), ('uhez ke',), ('uhoM ke',), ('uhoz ke',)\n",
    "]\n",
    "\n",
    "# Prepare the output structure\n",
    "categories = []\n",
    "word_roots = []\n",
    "\n",
    "# Process the input data\n",
    "for entry in pronoun_data:\n",
    "    word_root = entry[0]\n",
    "    categories.append(\"Pronoun\")  # Assign category as Pronoun\n",
    "    word_roots.append(word_root)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Category': categories,\n",
    "    'Word Root': word_roots\n",
    "})\n",
    "\n",
    "# Save to Excel\n",
    "df.to_excel('word_data_pronoun.xlsx', index=False)\n",
    "\n",
    "print(\"Data has been saved to 'word_data_pronoun.xlsx'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "385c8086-ca04-4015-820f-9e2d3cb91fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV Headers: dict_keys(['\\ufeffCategory', 'Root', 'Word Form 1', 'Word Form 2', 'Word Form 3', 'Word Form 4'])\n",
      "Done file has been save in bhojpuri.dix\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import List, Dict\n",
    "\n",
    "class ApertiumDixConverter:\n",
    "    def __init__(self, dix_template: str):\n",
    "        self.tree = ET.parse(dix_template)\n",
    "        self.root = self.tree.getroot()\n",
    "\n",
    "    def add_entry(self, category: str, root: str, word_forms: List[str]):\n",
    "        paradigm_name = f\"{category}_{root}\"\n",
    "        e = ET.Element(\"e\")\n",
    "        e.set(\"lm\", root)\n",
    "        p = ET.SubElement(e, \"paradigm\")\n",
    "        p.set(\"name\", paradigm_name)\n",
    "        for form, (number, case) in zip(word_forms, [(\"sg\", \"nom\"), (\"sg\", \"obl\"), (\"pl\", \"nom\"), (\"pl\", \"obl\")]):\n",
    "            for variant in form.split(\"/\"):\n",
    "                l = ET.Element(\"l\")\n",
    "                l.text = variant\n",
    "                n = ET.SubElement(l, \"n\")\n",
    "                n.set(\"n\", number)\n",
    "                n.set(\"c\", case)\n",
    "                e.append(l)\n",
    "        self.root.append(e)\n",
    "\n",
    "    def save(self, output_file: str):\n",
    "        tree_str = ET.tostring(self.root, encoding=\"unicode\")\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(tree_str)\n",
    "\n",
    "def read_csv(file_path: str) -> List[Dict[str, str]]:\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        data = [{k.strip(): v.strip() for k, v in row.items()} for row in reader]\n",
    "    \n",
    "    print(\"CSV Headers:\", data[0].keys())  # Debugging: print actual headers\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_file = \"word_data.csv\"\n",
    "    dix_template = \"hindi.dix\"\n",
    "    output_dix = \"bhojpuri.dix\"\n",
    "    \n",
    "    data = read_csv(csv_file)\n",
    "    converter = ApertiumDixConverter(dix_template)\n",
    "    \n",
    "    for row in data:\n",
    "        category, root = row[\"\\ufeffCategory\"], row[\"Root\"]\n",
    "        word_forms = [row[f\"Word Form {i}\"] for i in range(1, 5)]\n",
    "        converter.add_entry(category, root, word_forms)\n",
    "    \n",
    "    converter.save(output_dix)\n",
    "    print(f\"Done file has been save in {output_dix}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cb710cea-355d-4d7a-af0c-108b791811a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from typing import List, Dict\n",
    "\n",
    "class ApertiumDixConverter:\n",
    "    def __init__(self, dix_template: str):\n",
    "        self.tree = ET.parse(dix_template)\n",
    "        self.root = self.tree.getroot()\n",
    "\n",
    "    def add_entry(self, paradigm: str, af_string: str, variants: List[str]):\n",
    "        af_parts = af_string.split(',')\n",
    "        if len(af_parts) < 8:\n",
    "            raise ValueError(\"Invalid af format\")\n",
    "\n",
    "        root, category, gender, number, person, case, vib, tam = af_parts[:8]\n",
    "        paradigm_name = f\"{paradigm}_{root}\"\n",
    "        \n",
    "        e = ET.Element(\"e\")\n",
    "        e.set(\"lm\", root)\n",
    "\n",
    "        sdefs = ET.SubElement(e, \"sdefs\")\n",
    "        for attr, value in zip([\"cat\", \"gend\", \"num\", \"per\", \"case\", \"vib\", \"tam\"], af_parts[1:]):\n",
    "            sdef = ET.SubElement(sdefs, \"sdef\")\n",
    "            sdef.set(\"n\", attr)\n",
    "            sdef.set(\"v\", value)\n",
    "        \n",
    "        if paradigm == \"Noun_m_e\":\n",
    "            sdefs.append(ET.Element(\"sdef\", {\"n\": \"e\", \"v\": \"emphatic_\"}))\n",
    "        elif paradigm == \"Noun_m_e1\":\n",
    "            sdefs.append(ET.Element(\"sdef\", {\"n\": \"e2\", \"v\": \"emphatic_\"}))\n",
    "        \n",
    "        for variant in variants:\n",
    "            l = ET.Element(\"l\")\n",
    "            l.text = variant\n",
    "            e.append(l)\n",
    "        \n",
    "        self.root.append(e)\n",
    "    \n",
    "    def save(self, output_file: str):\n",
    "        tree_str = ET.tostring(self.root, encoding=\"unicode\")\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(tree_str)\n",
    "\n",
    "# Example usage\n",
    "dix_template = \"hindi.dix\"\n",
    "output_dix = \"bhojpurei.dix\"\n",
    "converter = ApertiumDixConverter(dix_template)\n",
    "\n",
    "# Sample entries\n",
    "entries = [\n",
    "    (\"Noun_m\", \"XX,n,m,sg,3,d,0,0\", [\"\", \"\"]),\n",
    "    (\"Noun_m_e\", \"XX,n,m,pl,3,o,0,0\", [\"\", \"_\"]),\n",
    "    (\"Noun_m_e1\", \"XX,n,m,sg,3,d,0,0\", [\"\", \"_\"])\n",
    "]\n",
    "\n",
    "for paradigm, af, words in entries:\n",
    "    converter.add_entry(paradigm, af, words)\n",
    "\n",
    "converter.save(output_dix)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5136190f-b60b-48b7-96d5-6ab226effd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def convert_txt_to_csv(txt_file, csv_file):\n",
    "    with open(txt_file, 'r', encoding='utf-8') as infile, open(csv_file, 'w', encoding='utf-8', newline='') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerow([\"ID\", \"Index\", \"Word\", \"Paradigm\", \"Meaning\", \"Author\"])  # CSV Headers\n",
    "        \n",
    "        for line in infile:\n",
    "            parts = line.strip().split(',')\n",
    "            if len(parts) == 6:  # Ensure correct format\n",
    "                writer.writerow(parts)\n",
    "            else:\n",
    "                print(f\"Skipping malformed line: {line}\")\n",
    "\n",
    "# Usage Example\n",
    "convert_txt_to_csv('dict.final.txt', 'lexicon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4ccb0165-799b-4d0f-b1a8-3880795f019a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading Data...\n",
      " Loaded Paradigms: 8\n",
      " Loaded Lexicon: 124\n",
      " Loaded Morph Categories: 0\n",
      " Generating Dictionary...\n",
      " Checking paradigms before writing XML:\n",
      "{'Noun_m': {'root': '', 'word_forms': ['', '', '', '']}, 'Noun_m_e': {'root': '', 'word_forms': ['//', '//', '//', '//']}, 'Noun_m_e1': {'root': '', 'word_forms': ['//', '//', '//', '//']}, 'Noun_f': {'root': '', 'word_forms': ['', '', '', '']}, 'Noun_f_e': {'root': '', 'word_forms': ['', '', '', '']}, 'Noun_f_rednt': {'root': '', 'word_forms': ['', '', '', '/']}, 'Noun_f_rednt_e': {'root': '', 'word_forms': ['//', '//', '//', '//']}, 'Noun_f_rednt_e1': {'root': '', 'word_forms': ['//', '//', '//', '//']}}\n",
      " XML successfully generated and saved as 'bhojpuri1.dix'!\n",
      " Done!\n",
      " Done bro!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "\n",
    "class DixGenerator:\n",
    "    def __init__(self, paradigm_file, lexicon_file, morph_file, output_file):\n",
    "        self.paradigm_file = paradigm_file\n",
    "        self.lexicon_file = lexicon_file\n",
    "        self.morph_file = morph_file\n",
    "        self.output_file = output_file\n",
    "        self.paradigms = {}  # Stores paradigm rules\n",
    "        self.lexicon = defaultdict(list)  # Stores word-paradigm mappings\n",
    "        self.morph_categories = defaultdict(dict)  # Stores morph categories\n",
    "\n",
    "    def load_paradigms(self):\n",
    "        \"\"\"Loads paradigm data from a CSV file\"\"\"\n",
    "        with open(self.paradigm_file, 'r', encoding='utf-8-sig') as f:\n",
    "            reader = csv.reader(f, delimiter=',')\n",
    "            next(reader, None)  # Skip header\n",
    "        \n",
    "            for row in reader:\n",
    "                if len(row) < 2:\n",
    "                    print(f\" Skipping malformed row: {row}\")\n",
    "                    continue\n",
    "            \n",
    "                paradigm, root, *word_forms = row\n",
    "                self.paradigms[paradigm] = {'root': root, 'word_forms': word_forms}\n",
    "        print(\" Loaded Paradigms:\", len(self.paradigms))\n",
    "\n",
    "    def load_lexicon(self):\n",
    "        \"\"\"Loads lexicon data from a CSV file\"\"\"\n",
    "        try:\n",
    "            with open(self.lexicon_file, 'r', encoding='utf-8') as f:\n",
    "                reader = csv.reader(f)\n",
    "                next(reader, None)  # Skip header if exists\n",
    "                for row in reader:\n",
    "                    try:\n",
    "                        _, _, word, paradigm, *_ = row  # Avoid index errors\n",
    "                        self.lexicon[paradigm].append(word)\n",
    "                    except IndexError:\n",
    "                        print(f\" Skipping malformed lexicon row: {row}\")\n",
    "            print(\" Loaded Lexicon:\", len(self.lexicon))\n",
    "        except FileNotFoundError:\n",
    "            print(f\" ERROR: File '{self.lexicon_file}' not found!\")\n",
    "        except Exception as e:\n",
    "            print(f\" ERROR in load_lexicon: {e}\")\n",
    "\n",
    "    def load_morph_categories(self):\n",
    "        \"\"\"Loads morphological categories from a text file\"\"\"\n",
    "        try:\n",
    "            current_category = None\n",
    "            with open(self.morph_file, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if line.startswith('[[') and line.endswith(']]'):\n",
    "                        current_category = line[2:-2].strip()\n",
    "                    elif current_category and '::' in line:\n",
    "                        parts = line.split('::')\n",
    "                        if len(parts) >= 2:\n",
    "                            morph_features, word_form = parts[:2]\n",
    "                            word_form = word_form.strip()\n",
    "                            morph_features = morph_features.strip()\n",
    "                            emph_suffix = \"_\" if \"emph='y'\" in morph_features else \"_\" if \"emph1='y'\" in morph_features else \"\"\n",
    "                            self.morph_categories[current_category][word_form + emph_suffix] = morph_features\n",
    "            print(\" Loaded Morph Categories:\", len(self.morph_categories))\n",
    "        except FileNotFoundError:\n",
    "            print(f\" ERROR: File '{self.morph_file}' not found!\")\n",
    "        except Exception as e:\n",
    "            print(f\" ERROR in load_morph_categories: {e}\")\n",
    "\n",
    "    def generate_xml(self):\n",
    "        \"\"\"Generates XML from the loaded data and saves it to a file\"\"\"\n",
    "        print(\" Checking paradigms before writing XML:\")\n",
    "        print(self.paradigms)  # Debugging print\n",
    "\n",
    "        root = ET.Element(\"dictionary\")\n",
    "        section = ET.SubElement(root, \"section\", name=\"main\")\n",
    "        \n",
    "        if not self.paradigms:  # If dictionary is empty\n",
    "            print(\" No paradigms loaded! Check CSV parsing.\")\n",
    "            return\n",
    "\n",
    "        for paradigm, data in self.paradigms.items():\n",
    "            entry = ET.SubElement(section, \"entry\", category=paradigm)\n",
    "            root_element = ET.SubElement(entry, \"root\")\n",
    "            root_element.text = data['root']\n",
    "            \n",
    "            for idx, form in enumerate(data['word_forms']):\n",
    "                variations = form.split('/')  # Split multiple variations\n",
    "                for var in variations:\n",
    "                    form_element = ET.SubElement(entry, f\"word_form_{idx+1}\")\n",
    "                    form_element.text = var.strip()  # Remove spaces\n",
    "        \n",
    "        tree = ET.ElementTree(root)\n",
    "        tree.write(self.output_file, encoding=\"utf-8\", xml_declaration=True)\n",
    "        print(f\" XML successfully generated and saved as '{self.output_file}'!\")\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Runs the entire pipeline\"\"\"\n",
    "        print(\" Loading Data...\")\n",
    "        self.load_paradigms()\n",
    "        self.load_lexicon()\n",
    "        self.load_morph_categories()\n",
    "        print(\" Generating Dictionary...\")\n",
    "        self.generate_xml()  # FIXED function name here\n",
    "        print(\" Done!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    generator = DixGenerator(\n",
    "        'word_data.csv', \n",
    "        'lexicon.csv', \n",
    "        'B_M_M_Word-generation-ver-1.9.0.formatted-structured-for-csv-for-dix-generation.txt', \n",
    "        'bhojpuri1.dix'\n",
    "    )\n",
    "    generator.run()\n",
    "    print(\" Done bro!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "87074b5e-cb2e-4ea7-acd2-a7b045032c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " XML file successfully created: bhojpuri_output23.xml\n",
      "Done Bro\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "\n",
    "def parse_dix_file(input_file, output_file):\n",
    "    try:\n",
    "        # Load and parse the dictionary XML file\n",
    "        tree = ET.parse(input_file)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # Initialize dictionary to store parsed data\n",
    "        parsed_data = {}\n",
    "        \n",
    "        for entry in root.findall(\".//entry\"):\n",
    "            category = entry.get(\"category\")\n",
    "            root_text = entry.find(\"root\").text if entry.find(\"root\") is not None else \"\"\n",
    "            \n",
    "            word_forms = []\n",
    "            for word_form in entry.findall(\"word_form_*\"):\n",
    "                if word_form.text:\n",
    "                    word_forms.append(word_form.text)\n",
    "            \n",
    "            parsed_data[category] = {\"root\": root_text, \"word_forms\": word_forms}\n",
    "        \n",
    "        # Convert parsed data to structured XML format\n",
    "        new_root = ET.Element(\"Dictionary\")\n",
    "        \n",
    "        for category, data in parsed_data.items():\n",
    "            noun_element = ET.SubElement(new_root, \"Noun\", category=category)\n",
    "            root_element = ET.SubElement(noun_element, \"Root\")\n",
    "            root_element.text = data[\"root\"]\n",
    "            \n",
    "            word_forms_element = ET.SubElement(noun_element, \"WordForms\")\n",
    "            for form in data[\"word_forms\"]:\n",
    "                word_element = ET.SubElement(word_forms_element, \"Word\")\n",
    "                word_element.text = form\n",
    "        \n",
    "        # Save the new XML file\n",
    "        tree = ET.ElementTree(new_root)\n",
    "        tree.write(output_file, encoding=\"utf-8\", xml_declaration=True)\n",
    "        print(f\" XML file successfully created: {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\" Error processing file: {e}\")\n",
    "\n",
    "# Input and output file names\n",
    "input_dix_file = \"bhojpuri1.dix\"\n",
    "output_xml_file = \"bhojpuri_output23.xml\"\n",
    "\n",
    "# Run the parser\n",
    "parse_dix_file(input_dix_file, output_xml_file)\n",
    "print(\"Done Bro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae823651-ba89-41b9-a9cd-9c333897f029",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
